---
title: "ACrep_Results.Rmd"
author: "Hannah Moshontz"
date: "Last Updated `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

## Purpose

This script generates text for the results section of the ACREP / PSA004 Stage 2 RRR. It also runs the analysis .Rmd.

# Run the analysis script

```{r, child = '../05_Analysis/ACrep_Analysis_V2.Rmd', results = FALSE}
```

```{r}
library(janitor)
library(stringr)
library(kableExtra)
library(countrycode)
library(ISOcodes)
library(tidylog)
```


# EDA on the analysis set

```{r}
final_long %>% 
  glimpse()
```


A few notes about variables:
- Person is a unique lab identifier (identifies PI)
- lab_id is a not a unique lab identifier, but rather identifies the undergraduate PI
(and some of these undergraduate PIs were students with the same lab PI)
- lab_country is the variable we are using to identify country/region
- UN_Region groups lab country into larger regions
- source identifies the method of data collection
- id uniquely identifies each participant by concatenating the source (qualtrics vs socscisurvey),
undergraduate PI (lab_id), and the unique identifier (case, which was of a different format depending on source)
- variables ending in valid are logical and identify valid cases (in the "final_long" dataset, there will not be variance)
- variables ending in order indicate the order in which vignettes or condition were presented (varies by participant),
or the order in which binary DVs were presented (for socscisurvey, this varies by person within lab such that people 
were presented with "normal" (*not sure what direction this is*) or "swapped", but for Qualtrics, this was randomized 
within person and so is coded as "mixed" and and we do not have more detailed information about order in the final dataset. also one lab didn't properly randomize.)
- 2104 N are missing age. are these likely test observations? and so this exclusion and over 100 were a way to try to catch the test observations.

```{r}
full_long %>% filter(is.na(UN_Region)) %>% tabyl(lab_id)
```
Information that came from lab_sheet (ultimately the source was a google doc) is not accurate and doesn't contain information about qualtrics labs. This is a problem.


# Method


```{r}

#labs that applied
n_labs_applied <- 69
n_labs_rejected <- (69-37)
n_labs_failed <- 30
n_labs_finished_crep <- 22
# sources: https://docs.google.com/spreadsheets/d/1Y4ZFB0AZQMhjVwGP-aA9eEC2CWKUUq5LdJu7wZLAdQ8/edit#gid=575778621
# https://docs.google.com/spreadsheets/d/1Y4ZFB0AZQMhjVwGP-aA9eEC2CWKUUq5LdJu7wZLAdQ8/edit#gid=1889366473

#create table content

#save teams per site
teams_per_site <- final_long %>% 
  group_by(Person, lab_id) %>% group_by(Person, lab_id) %>% count() %>% 
  group_by(Person) %>% count() %>%
  rename(n_student_teams = n) %>% 
  ungroup() %>% 
  glimpse()

#save full n per site
full_data_n_per_site <- full_long %>% 
  group_by(Person, id) %>% count() %>% 
  group_by(Person) %>% count() %>% 
  rename(full_data_n = n) %>% 
  ungroup() %>% 
  glimpse()

#save final n per site
final_data_n_per_site <- final_long %>% 
  group_by(Person, id) %>% count() %>% 
  group_by(Person) %>% count() %>% 
  rename(final_data_n = n) %>% 
  ungroup() %>% 
  glimpse()

#check assumption that each site had one data collection source...
#error out if any site contributed data from both Qualtrics and SSS
if (full_long %>% 
  group_by(Person, source) %>% count() %>% 
  group_by(Person) %>% count() %>% 
  filter(n > 1) %>% 
  nrow() %>% .[1] != 0) STOP

#save survey platform info
platform <- full_long %>% 
  group_by(Person) %>% 
  summarize(survey_platform = first(source)) %>%
  ungroup() %>% 
  glimpse()

#save world region info
full_long %>% 
  group_by(Person)

#check assumption that each site had one compensation type...
#something is fishy
#FIX: Need to check all information in lab_sheet and it's source gsheet
multiple_compensation <- lab_sheet %>% 
   group_by(Person, Compensation) %>% count() %>% 
   group_by(Person) %>% count() %>% 
   filter(n > 1) %>% pull(Person)

#exporting summary of issue
lab_sheet %>% 
  filter(Person %in% multiple_compensation) %>% 
  select(lab_code, Person, Compensation, comp) 
  #write.csv("compensation_discrepancies.csv")

#save compensation information 
lab_sheet %>% 
  select(Person, Compensation) %>% 
  group_by(Person) %>% 
  count()

left_join(teams_per_site, full_data_n_per_site, by = "Person") %>% 
  left_join(final_data_n_per_site, by = "Person") %>% 
  left_join(platform, by = "Person") %>% 
arrange(-final_data_n) %>% 
  mutate(site_id = row_number()) %>% 
  select(site_id, everything()) %>% 
  kable(caption = "data collection sites") %>% 
  kable_styling()
```


In the approved protocol, sections of the manuscript relevant to understanding the study method included detailed descriptions of the study's logistics, including the recruitment and approval of collaborators who would collect data. This section has been restructured to more closely resemble a typical study method description, including relevant information about the study's logisitics. The text from the approved protocol has been updated and moved to Appendix X.

## Participants

This project was designed to be a training and learning experience for undergraduate students. It was conceived of and led by the Collaborative Replications and Education Project (CREP; Wagge et al., cite) in concert with the Psychological Science Accelerator (PSA; Moshontz et al., 2019). For the undergraduate students who served the role of researcher in this study, the project involved collecting data and completing a series of tasks with pedagogical value, like independetly designing an analysis plan, completing a preregistration, independently executing analyses, and posting the study materials, data, and results summaries in a central repository. Each student-led team submitted a protocol for review to a CREP reviewer. Teams could not contribute to data collection until the protocol was approved (for more information about this process, see Supplement X).

For the present article and analyses, data are structured such that observations are nested within data collection sites. At some sites, there were multiple student-led teams who completed the project. Because observations collected by different teams are organized within a single data collection site, counts of student-led teams sometimes differ from counts of data collection sites. Further, some teams that were approved to collect data did not contribute data to this project, either because they did not collect any data (e.g., due to campus closures during the Covid-19 pandemic) or because data were collected in a way that rendered them unsuable for analyses (e.g., vignettes were not appropriately randomized). For a summary of student-led team and data collection site information, see Supplement X.

In total, `r n_labs_applied` teams led by undergraduate students signed up to collect data for this project.  Of the teams that originally signed up, `r n_labs_rejected` either did not submit protocols for review or submitted protocols but did not complete required revisions. In total, `r n_labs_finished_crep` teams completed the entire project, including the purely pedagogical tasks. Some teams collected data but did not complete the project. Prior to exclusions, `r length(unique(full_demographics$Person))` unique data collection sites contributed data to the project. Data collection sites were located in `r length(unique(full_long$lab_country))` countries spanning world regions (i.e., `r count(group_by(full_long, UN_Region)) %>% drop_na() %>% arrange(-n) %>% pull(UN_Region) %>% paste(collapse = ", ")`). See Table X for a summary of student-led teams, and Table X for a summary of data collection sites.

```{r}
#create dataset for demographics 
final_demographics <- final_long %>% 
  group_by(id) %>%
  slice(1) %>% 
  ungroup() %>% 
  select(id, 
         source,
         contains("exclusion"),
         contains("nonsense"),
         purpose,
         previous,
         survey_lang,
         turk,
         language,
         lab_id, 
         Person,
         lab_country,
         country,
         UN_Region,
         gender, 
         gender_other,
         starts_with("ethn_"),
         education,
         education_level,
         starts_with("comp"),
         birth_country,
         case,
         age)

full_demographics <- full_long %>% 
  group_by(id) %>%
  slice(1) %>% 
  ungroup() %>% 
  select(id, 
         source,
         contains("exclusion"),
         contains("nonsense"),
         purpose,
         previous,
         survey_lang,
         turk,
         language,
         lab_id, 
         Person,
         lab_country,
         country,
         UN_Region,
         gender, 
         gender_other,
         starts_with("ethn_"),
         education,
         education_level,
         starts_with("comp"),
         birth_country,
         case,
         age)

#error out if count doesn't match
if (length(unique(final_long$id)) != nrow(final_demographics)) STOP

```



Of the `r nrow(full_demographics)` participants who completed the survey, data from `r nrow(full_demographics) - nrow(final_demographics)`% (_n_ = `r`) of people were excluded.

Data were excluded for the following reasons (applied in the order listed):
(1) the participant did not provide an age, listed an age greater than or equal to 100, or was not the majority age of their country or older,  operationalized as at least 18 in all countries except Slovakia, where the age of consent is 19, and United Arab Emirates, where the age of consent is 21 (_n_ = `r nrow(filter(full_demographics, age_exclusion))`);
(2) the participant had taken part in a previous version of this study or in another contributors' replication of the same study (_n_ = `r nrow(filter(full_demographics, previous_exclusion))`);
(3) the participant failed to answer all three of the vignette comprehension questions correctly (_n_ = `r nrow(filter(full_demographics, studyans_exclusion)))`; e.g., did not correctly identify what Darrel was looking at);
(4) the participant correctly and explicitly articulated knowledge of the specific hypotheses or specific conditions of this study when asked what they thought the study hypothesis was (_n_ = `r nrow(filter(full_demographics, purpose_exclusion))`);
(5) the participant reported their understanding of the language the survey was written in as "not well" or "not well at all" (_n_ = `r nrow(filter(full_demographics, lang_exclusion))`); see Vickstrom, Shin, Collazo, & Bauman, 2015).

Many excluded observations met multiple criteria.

```{r}
exclusions %>% 
  kable(caption = "Multiway Table of All Exclusions") %>% 
  kable_styling()

#Note: There are many NA in exclusion criteria because we didn't evaluate folks that were excluded for previous criteria
```


Evaluating criteria 2 and 4 required making subjective judgements about open-ended responses. Each non-missing observation was evaluated by three people who spoke the langauge. These three raters did not translate responses, but instead directly evaluated responses with respect to the exclusion criteria (see the Supplement for the instructions given to raters and information about how ratings were combined). Ratings themselves are available at osf.io/gs29c. Responses marked as identified test cases (e.g., "TEST") were excluded. Responses that were not coherent were labeled, but not excluded, and some observations in the final data were not coherent (_n_ study purpose = `r nrow(filter(final_demographics, purpose_nonsense))`; _n_ previously participated = `r nrow(filter(final_demographics, previous_nonsense))`). Responses that were missing were not evaluated by raters, but no observations in the final dataset were missing evalutions (_n_ study purpose = `r nrow(filter(final_demographics, is.na(purpose_exclusion)))`; _n_ previously participated = `r nrow(filter(final_demographics, is.na(previous_exclusion)))`). 

All of these exclusions were preregistered with one exception. We did not pregister the exclusion of people over the age of 100, which resulted in the excusion of data from `r nrow(filter(full_demographics, !(age < 100)))` people. The age exclusion was made first, and captured many observations in the data that appeared to be invalid test responses. Data collection sites were not given instructions about avoiding or clearly identifying test responses. At many data collection sites, the students and other researchers executing the study tested their survey link multiple times (e.g., as inferred by responses to open-ended questions marked "test"). The exclusion of people over the age of 100 likely captured many invalid test responses. 


In the analysis sample (i.e., following exclusions), participants were `r nrow(final_demographics)` adults recruited to participate by student researchers at `r length(unique(final_long$Person))` data collection sites in `r length(unique(final_long$lab_country))` countries spanning world regions (i.e., `r count(group_by(final_long, UN_Region)) %>% drop_na() %>% arrange(-n) %>% pull(UN_Region) %>% paste(collapse = ", ")`). Data collection sites contributed an average of `r final_demographics %>% group_by(Person) %>% count() %>% pull(n) %>% mean() %>% round(2)` participants to the anlaysis sample (min = `r final_demographics %>% group_by(Person) %>% count() %>% pull(n) %>% min()`, max = `r final_demographics %>% group_by(Person) %>% count() %>% pull(n) %>% max()`). See Table X for a summary of participants' age, years of education, and ethnic/racial identity.

Participation details, like compensation and the sampled population, varied by data collection site, as summarized in Table X. In the approved protocol, we planned to collect all data using a single SocSciSurvey survey programmed to accomodate lab-specific variations. However, some used Qualtrics surveys that student researchers programmed themselves. Most data collection sites used the central SocSciSurvey survey.

```{r}
final_demographics %>% 
  select(age, education) %>% 
  describe() %>% 
  .[1:2,c(2,3,4,8,9)] %>% 
  kable(digits = 2) %>% 
  kable_styling()
```

In the approved proposal, we planned to measure participants' racial and ethnic identities using an open-ended response. For reasons that were not documented, racial and ethnic identity was measured using non-exclusive cateogories with an open-ended fill-in option. Student research teams designed different response options tailored to their geographic region (see all variations in Supplemental Table X). All data collection sites allowed people to select multiple racial and ethnic identities, and all asked whether participants identified as White (either "White/European", "White / European descent", or "European descent"). In the final sample, `r round(nrow(filter(final_demographics, ethn_wh == TRUE))/nrow(final_demographics)*100, 2)`% (_n_ = `r nrow(filter(final_demographics, ethn_wh == TRUE))`) identified as White.


```{r}
final_demographics %>% 
  select(comp, turk, source) %>% 
  mutate(source = as.factor(source)) %>% 
  mutate(across(.cols = everything(), ~as.numeric(.x))) %>% 
  mutate(comp = comp - 1,
         `Took the centralized survey` = source - 1) %>%  #adjusting numeric values so 0 is no and 1 is yes
  select(-source) %>% 
  rename(`Recruited through Mturk` = turk,
         `Compensated for participation` = comp) %>% 
  describe() %>% 
  .[,c(2,3,4,8,9)] %>% 
  mutate(count = n*mean,
         percent = count/n*100) %>% 
  select(count, percent) %>% 
  kable(digits = 2, caption = "Number and Percent of Participants by Data Collection Context Variables (Total N = 2957)") %>% 
  kable_styling()

```


```{r}
final_demographics %>% 
  mutate(lab_country_full = countrycode(lab_country, "iso3c", "country.name.en")) %>% 
  tabyl(lab_country_full) %>% 
  arrange(-n) %>% 
  kable(digits = 2, caption = "Number and Percent of Participants by Country") %>% 
  kable_styling()
```

```{r}
survey_lang_table <- final_demographics %>%
  mutate(survey_lang = str_replace_all(survey_lang, "prt", "por"),
         survey_lang = str_replace_all(survey_lang, "zho", "chi")) %>% 
  tabyl(survey_lang) %>% 
  arrange(-n)

for (i in 1:length(survey_lang_table$survey_lang)) {
  
  survey_lang_table$survey_lang[i] <- ISO_639_2$Name[ISO_639_2$Alpha_3_B == survey_lang_table$survey_lang[i]]
  
}

survey_lang_table %>% 
  mutate(survey_lang = str_replace_all(survey_lang, "Greek, Modern \\(1453\\-\\)", "Greek"),
         survey_lang = str_replace_all(survey_lang, "Romanian; Moldavian; Moldovan", "Romanian")) %>% 
  kable(digits = 2, caption = "Number and Percent of Participants by Survey Language") %>% 
  kable_styling()
```


## Procedure

Data collection took place between January 1, 2019 and June 1, 2021. Some participants completed the study survey from a campus laboratory. All participants completed an online survey that included a consent form. After providing informed consent, participants read and answered questions about three vignettes whose content and conditions varied according to the random presentation feature in SocSciSurvey and Qualtrics. The vignettes used in the study were selected on the basis of pretesting in English, described in Appendix X. 

Vignette content varied by subject and identification target. In one scenario, Darrel identified prarie dogs vs red speckled ground squirrels (cite original). In another scenario, .... In a third scenario, .... Some student researchers opted to include a fourth scenario that was not included in the present analyses but can be found in Appendix X. 

After each vignette, participants evaluated: 1) whether the subject believes or knows, measured either on a continuous scale from 0 to 100 or as a binary choice;
2) the true correct answer, which functioned as a comprehension question and was measured as a binary choice between the two identification options;
3) the extent to which the subject's belief was unreasonable or reasonable, measured either on a continuous scale from 0 to 100 or as a binary choice;
4) whether or not the subject correctly identified the target.




# Disclosures


*Data, materials, and online resources:* Deidentified raw data and deidentified data with exclusions are are posted publicly on our master OSF page (https://osf.io/n5b3w/), and each contributing site has posted their data on an OSF page linked to our master OSF page.


*Reporting:*

 “We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study” (see Simmons, Nelson, & Simonsohn, 2011).
 
*Ethical approval:*

All contributing labs were required to submit their local institutional ethics approval prior to data collection as part of their pre-registstration and CREP review process and will be carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki. All participating labs posted their ethics approval to their lab's OSF page for this study. 

*Author Contributions:*

TBD

*Conflicts of Interest:*

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

*Funding:*

TBD

*Supplemental Material:*

If Supplemental Material will be posted on the journal’s Web site, include this heading and the appropriate link will be added during editing.

*Prior versions:*

If part or all of a submitted manuscript was previously posted to a blog or to a preprint archive, the authors should provide a link to that source and briefly indicate what aspects of the submitted manuscript are shared with that prior version.

# Supplement

### Exclusion ratings

**How exclusions were made**

No or NA gets 0 points 
Maybe gets 1 point
Yes/test gets 2 points
Participants are marked as "excluded" if they get 4 total points across three coders 

Participants are also marked as "nonsense" if they did not write a legible answer or simply typed gibberish. These data points are not excluded but marked.

**Instructions Given to Raters**

Note: These instructions were adapted from instructions written by William McAuliffe and Hannah Moshontz for an unrelated project

We need help coding open-ended responses that will inform our pre-registered exclusion criteria for this project. Specifically, we will exclude data on the basis of a suspicion check (whether people guess the study hypothesis) and previous study participation (whether people describe having participated in similar studies before). 

All participants were asked two questions (with some labs asking slight variations):
What is, in your opinion, the purpose of this study? (purpose)
Have you ever participated in a similar study? If yes, please describe the study. (previous)

Your task is to evaluate people’s answers to these questions. We will have 2 people evaluate every response and then we will exclude people based on the average.

For each question this is how we would like you to evaluate answers. If you are coding from a language other than English, please directly assess the question (rather than translating it) and provide a code/label in English (yes, maybe, no, test, as described below).

**purpose**

_yes_

The participant identified that we are studying justified true belief and gettier cases.

Example “yes” coding cases: “To test exceptions to the Justified True Belief theory”

_maybe_

The participant describes something similar to the true study hypothesis (true knowledge is different from a lucky or incidentally correct belief). 

Example “maybe” coding cases: “To see if a story can change ones perception of knowledge based on luck or ability”

_no_

The participant did not identify the study hypothesis or offer a very vague description, which might include the words belief or knowledge. 

Example “no” coding cases: “I think the purpose was to see how do people classify if someone knows something or if they just strongly agree with it”; OR “understand how people view scenarios based on the words used to describe them"

_test_ 

The response indicates that it is a test case    

Example “test” coding case: “TEST”; OR “test”; OR “this is a test”

_NA_ 

If you are unsure how to code a response, you can write NA.

Example “NA” coding case: “nnnnnnnnnnn”

**Previous**

_yes_ 

The participant has participated in this exact study before, or an exact replication of it.

Example “yes” coding cases: “Yes, I completed this study before.” 

_maybe_ 

The participant has participated in a similar study before, or may have based on their description.

Example “maybe” coding cases:  “Yes another study that was very similar.”; “Yes, I have participated in a study that asked similar questions but had slightly different scenarios”

_no_ 

The participant has not participated in this study or a similar study before based on their description.

Example “no” coding cases: “nope”; “Yes, I have participated in a study for course credit before.”’; “Yes, I have done studies where I read scenarios and answered questions about them.”

_test_ 

The response indicates that it is a test case        

Example “test” coding case: “TEST”; OR “test”; OR “this is a test”

_NA_ 

If you are unsure how to code a response, you can write NA.

Example “NA” coding case: “nnnnnnnnnnn”

**Do’s and Don’ts**
Take breaks! This work is hopefully interesting, but it can be cognitively exhausting. If you are having trouble paying attention while you are doing this or if you feel tired of it, please take a break. 

After you label a response, do not go back and change it later. This may be tempting to do after mentally comparing how you rated different responses, but just carefully work through each response and know that your initial rating is final.

Don’t discuss your ratings with other raters—this will invalidate everyone’s work.
Do assign labels for every response in your assigned sheet(s). If you would like to contribute more, please email the person listed at the top of this sheet.

**To summarize, for each set of answers**
Read the answer to the question and assign a label that describes either whether people intuited the study hypothesis (for purpose) or whether people participated in a similar study previously (for previous)

**Coding form includes**
[id] A subject id number
[survey_lang]
[purpose/previous] The answer people gave to the question
[code] The code/label that you are assigning to the answer (yes, no, maybe, test)



### Demographics

*One version in SSS had these response options:*
White / European descent
Black / African descent
Latino*a / Latin American descent
Australian descent
Asian
Southeast Asian descent
Native American
Hawaiian descent / Pacific Islands
Other

*The other version in SSS had these response options:*
European descent
African descent
Latino*a / Latin American descent
Indigenous Australian or Torres Strait Islander descent
East Asian descent
South Asian descent
Pacific Island descent
Native American descent

*In Qualtrics these were the response options:*
White/European
Black/African American
Hispanic Latino
East or Southeast Asian / Pacific Islander (e.g. from Japan, China, Korea, Vietnam, Thailand, Philippines, native Hawaiian)
South Asian (e.g. from India, Pakistan)
I prefer not to answer this question
Other

```{r}

```

