---
title: "ACREP Updated Analyses"
author: "erin buchanan"
date: "Last Updated `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 15)

#note -- may need to change the working directory if you default to project
#setwd("./05_Analysis")
```

## Information

Multilab close replication of: Experiment 1 from Turri, J., Buckwalter, W., & Blouw, P. (2015). Knowledge and luck. *Psychonomic Bulletin and Review*, 22, 378-390.

[Data and registered protocols:](https://osf.io/n5b3w/)

[Codebook (raw data)](https://docs.google.com/spreadsheets/d/1KjXqgfVgguHeDXVtlHHhJ9zsRGVDfPVPH4tbh75P46U/edit#gid=903093128)

[Preprint](https://psyarxiv.com/zeux9/)

## Libraries

```{r}
library(rio)
library(lme4)
library(lmerTest)
library(nlme)
library(MuMIn)
library(plyr)
library(psych)
library(tidyr)
library(ggplot2)
library(dplyr)
library(ggmosaic)
library(MOTE)
```

## Import the Data

The `full_long` dataset includes all participants in long format - wherein each trial of their study is on one row of the dataset. Our uploaded data also includes `full.Rds` which is the same data in wide format - one column for each variable in the dataset and one row per participant.

```{r}
full_long <- readRDS("../04_Data/rds/d_all_long.Rds")
#str(full_long)
```

## Open Ended Response Coding 

Import the open-ended responses and prepare them for merging with the final dataset. This section:

  - Imports all the files
  - Includes a coder id
  - Corrects all the issues with typed codings
  - Converts each item to coder counts so you can convert from wide to long
  - Converts from wide to long
  - Converts into scoring:
    - No or NA gets 0 points 
    - Maybe gets 1 point
    - Yes/test gets 2 points
    - Participants are marked as "excluded" if they get 4 total points across three coders 
  - Participants are also marked as "nonsense" if they did not write a legible answer or simply typed gibberish. These data points are not excluded but marked if you would like to repeat the analysis and exclude them. 

```{r}
file.names <- list.files(path = "../04_Data/open_responses/", 
                         pattern = ".xlsx", full.names = T, 
                         recursive = T)

previous.files <- lapply(file.names, function(x){ import(x, sheet = 2)})
purpose.files <- lapply(file.names, function(x){ import(x, sheet = 3)})

# add coder ids 
coder.id <- unlist(lapply(strsplit(file.names, "psa004"), function(x) x[[2]]))

names(previous.files) <- coder.id
names(purpose.files) <- coder.id

previousDF <- do.call(rbind.fill, previous.files)
purposeDF <- do.call(rbind.fill, purpose.files)

previousDF$coder <- rep(names(previous.files), unlist(lapply(previous.files, nrow)))
purposeDF$coder <- rep(names(purpose.files), unlist(lapply(purpose.files, nrow)))

# ensure the text is normalized 
previousDF$previous <- gsub("\\s+", " ", previousDF$previous)
purposeDF$purpose <- gsub("\\s+", " ", purposeDF$purpose)
previousDF$previous <- gsub("\\s$", "", previousDF$previous)
purposeDF$purpose <- gsub("\\s$", "", purposeDF$purpose)

# can't figure out why these two are different
purposeDF$purpose[purposeDF$id == "s10313"] <- "I'm not sure."
purposeDF$purpose[purposeDF$id == "s10513"] <- "Recall, recognizing"

# fix the yes/no/maybe/NA
previousDF$code <- tolower(previousDF$code)
table(previousDF$code)
previousDF$code <- gsub("np", "no", previousDF$code)
previousDF$code <- gsub("n/a", "na", previousDF$code)
previousDF$code <- gsub("y$", "yes", previousDF$code)
table(previousDF$code)
previousDF <- subset(previousDF, !is.na(id))

purposeDF$code <- tolower(purposeDF$code)
table(purposeDF$code)
purposeDF$code <- gsub("mayb$|meybe", "maybe", purposeDF$code)
purposeDF$code <- gsub("mo", "no", purposeDF$code)
purposeDF$code <- gsub("n/a", "na", purposeDF$code)
table(purposeDF$code)
purposeDF <- subset(purposeDF, !is.na(id))

# add coder count
previous_count <- as.data.frame(table(previousDF$id))
previous_count <- previous_count[order(previous_count$Var1), ]
previousDF <- previousDF[order(previousDF$id), ]
previousDF$coder_num <- NA

for (i in 1:length(previous_count$Freq)){
  previousDF$coder_num[previousDF$id == previous_count$Var1[i]] <- 1:previous_count$Freq[i]
}

purpose_count <- as.data.frame(table(purposeDF$id))
purpose_count <- purpose_count[order(purpose_count$Var1), ]
purposeDF <- purposeDF[order(purposeDF$id), ]
purposeDF$coder_num <- NA

for (i in 1:length(purpose_count$Freq)){
  purposeDF$coder_num[purposeDF$id == purpose_count$Var1[i]] <- 1:purpose_count$Freq[i]
}

# long to wide for scoring
previousDF_wide <- pivot_wider(data = previousDF,
                              id_cols = c("id", "survey_lang", "previous"), 
                              names_from = "coder_num",
                              values_from = "code")

purposeDF_wide <- pivot_wider(data = purposeDF,
                              id_cols = c("id", "survey_lang", "purpose"), 
                              names_from = "coder_num",
                              values_from = "code")

# score the yes/no/maybe/NA
previousDF_wide$score_1 <- previousDF_wide$`1`
previousDF_wide$score_2 <- previousDF_wide$`2`
previousDF_wide$score_3 <- previousDF_wide$`3`

previousDF_wide$score_1 <- gsub("maybe", "1", previousDF_wide$score_1)
previousDF_wide$score_1 <- gsub("no|na", "0", previousDF_wide$score_1)
previousDF_wide$score_1 <- gsub("yes|test", "2", previousDF_wide$score_1)
previousDF_wide$score_2 <- gsub("maybe", "1", previousDF_wide$score_2)
previousDF_wide$score_2 <- gsub("no|na", "0", previousDF_wide$score_2)
previousDF_wide$score_2 <- gsub("yes|test", "2", previousDF_wide$score_2)
previousDF_wide$score_3 <- gsub("maybe", "1", previousDF_wide$score_3)
previousDF_wide$score_3 <- gsub("no|na", "0", previousDF_wide$score_3)
previousDF_wide$score_3 <- gsub("yes|test", "2", previousDF_wide$score_3)

previousDF_wide$count <- apply(
  previousDF_wide[ , grep("score", colnames(previousDF_wide))], 1, 
  function(x) { sum(!is.na(x)) } )
previousDF_wide$sum <- apply(
  previousDF_wide[ , grep("score", colnames(previousDF_wide))], 1, 
  function (x) { sum(as.numeric(x), na.rm = T)} )

previousDF_wide$exclude <- previousDF_wide$sum >= 4
previousDF_wide$nonsense <- apply(
  previousDF_wide[ , grep("1|2|3|4", colnames(previousDF_wide))], 1, 
  function (x) {sum(as.numeric(x == "na"), na.rm = T)}
) == 3

purposeDF_wide$score_1 <- purposeDF_wide$`1`
purposeDF_wide$score_2 <- purposeDF_wide$`2`
purposeDF_wide$score_3 <- purposeDF_wide$`3`

purposeDF_wide$score_1 <- gsub("maybe", "1", purposeDF_wide$score_1)
purposeDF_wide$score_1 <- gsub("no|na", "0", purposeDF_wide$score_1)
purposeDF_wide$score_1 <- gsub("yes|test", "2", purposeDF_wide$score_1)
purposeDF_wide$score_2 <- gsub("maybe", "1", purposeDF_wide$score_2)
purposeDF_wide$score_2 <- gsub("no|na", "0", purposeDF_wide$score_2)
purposeDF_wide$score_2 <- gsub("yes|test", "2", purposeDF_wide$score_2)
purposeDF_wide$score_3 <- gsub("maybe", "1", purposeDF_wide$score_3)
purposeDF_wide$score_3 <- gsub("no|na", "0", purposeDF_wide$score_3)
purposeDF_wide$score_3 <- gsub("yes|test", "2", purposeDF_wide$score_3)

purposeDF_wide$count <- apply(
  purposeDF_wide[ , grep("score", colnames(purposeDF_wide))], 1, 
  function(x) { sum(!is.na(x)) } )
purposeDF_wide$sum <- apply(
  purposeDF_wide[ , grep("score", colnames(purposeDF_wide))], 1, 
  function (x) { sum(as.numeric(x), na.rm = T)} )

purposeDF_wide$exclude <- purposeDF_wide$sum >= 4
purposeDF_wide$nonsense <- apply(
  purposeDF_wide[ , grep("1|2|3", colnames(purposeDF_wide))], 1, 
  function (x) {sum(as.numeric(x == "na"), na.rm = T)}
) == 3

sum(duplicated(purposeDF_wide$id))
#ids <- purposeDF_wide$id[duplicated(purposeDF_wide$id)]
#View(purposeDF_wide[purposeDF_wide$id %in% ids, ])
sum(duplicated(previousDF_wide$id))
#ids <- previousDF_wide$id[duplicated(previousDF_wide$id)]
#View(previousDF_wide[previousDF_wide$id %in% ids, ])
```

### Finding missing coders:

```{r}
previous_missing <- subset(previousDF_wide, count < 3)
purpose_missing <- subset(purposeDF_wide, count < 3)

table(previous_missing$survey_lang)
table(purpose_missing$survey_lang)

#create reviewer names
previous_completed <- previousDF %>% 
  group_by(id) %>% 
  summarize(coder = paste(coder, collapse = ", "))

purpose_completed <- purposeDF %>% 
  group_by(id) %>% 
  summarize(coder = paste(coder, collapse = ", "))

previous_missing <- merge(previous_missing, 
                          previous_completed, 
                          by = "id", 
                          all.x = T)

purpose_missing <- merge(purpose_missing, 
                         purpose_completed,
                         by = "id", 
                         all.x = T)

write.csv(previous_missing, "../04_Data/data/previous_needs.csv", row.names = F)
write.csv(purpose_missing, "../04_Data/data/purpose_needs.csv", row.names = F)
```

```{r}
previousDF_long_missing <- subset(previousDF, id %in% previous_missing$id)
previousDF_wide_missing <- pivot_wider(data = previousDF_long_missing, 
                                       id_cols = c("id", "survey_lang"), 
                                       values_from = "code", 
                                       names_from = "coder")

purposeDF_long_missing <- subset(purposeDF, id %in% purpose_missing$id)
purposeDF_wide_missing <- pivot_wider(data = purposeDF_long_missing, 
                                       id_cols = c("id", "survey_lang"), 
                                       values_from = "code", 
                                       names_from = "coder")

write.csv(previousDF_wide_missing, "../04_Data/data/previous_needs_wide.csv", row.names = F)
write.csv(purposeDF_wide_missing, "../04_Data/data/purpose_needs_wide.csv", row.names = F)
```

## Fix Data Coding / Examine Covariates  

### Deal with lab coding issues:

  - Import lab sheet
  - Delete cases that should not be used 

```{r} 
lab_sheet <- import("../04_Data/data/lab_sheet.csv")

nrow(full_long)

# delete test and do not use cases
full_long <- subset(full_long, lab_id %in% c(lab_sheet$lab_code[lab_sheet$Use == "Yes"], "87d100", "b6c8ec"))

nrow(full_long)

full_long$lab_id <- droplevels(full_long$lab_id)
```

### Deal with compensation issues:

  - Deal with compensation for labs with no information because they didn't see that question 
  
```{r}
# deal with compensation questions
# give compensation type labels
table(full_long$comp, useNA = "ifany")
full_long$comp <- factor(full_long$comp,
                         levels = 1:3,
                         labels = c("No", "Yes", "Not Sure"))

# examine compensation type by lab
table(full_long$lab_id, full_long$comp, useNA = "ifany")

# find labs that had majority no comp
lab_comp_temp <- as.data.frame(table(full_long$lab_id, full_long$comp, useNA = "ifany") / rowSums(table(full_long$lab_id, full_long$comp, useNA = "ifany")) * 100)

lab_comp_temp <- lab_comp_temp[order(lab_comp_temp$Freq, decreasing = T) , ]
lab_comp_max <- lab_comp_temp[!duplicated(lab_comp_temp$Var1), ]
lab_comp_max <- subset(lab_comp_max, is.na(Var2))
colnames(lab_comp_max) <- c("lab_code", "comp", "Percent")

# clean up lab codes compensation
lab_sheet$comp <- "Yes"
lab_sheet$comp[grepl("-", lab_sheet$Compensation)] <- "Unclear"
lab_sheet$comp[grepl("Not clear", lab_sheet$Compensation)] <- "Unclear"
lab_sheet$comp[grepl("Unsure", lab_sheet$Compensation)] <- "Unclear"
lab_sheet$comp[grepl("In Vivo", lab_sheet$Compensation)] <- "Mixed"
lab_sheet$comp[grepl("None", lab_sheet$Compensation)] <- "No"
lab_sheet$comp[is.na(lab_sheet$Compensation)] <- "Unclear"

table(lab_sheet$comp, useNA = "ifany")

# merge these two things together
lab_comp_max <- merge(lab_comp_max, lab_sheet, by = "lab_code")

# drop all unclear or mixed
lab_comp_max <- subset(lab_comp_max, comp.y != "Mixed" & 
                         comp.y != "Unclear")

# fix compensation for labs 
full_long$comp[full_long$lab_id %in% 
                 lab_comp_max$lab_code[lab_comp_max$comp.y == "Yes"]] <- "Yes"
full_long$comp[full_long$lab_id %in% 
                 lab_comp_max$lab_code[lab_comp_max$comp.y == "No"]] <- "No"

# this is mturk compensation is yes 
full_long$comp[full_long$lab_id == "b6c8ec"] <- "Yes"
full_long$turk <- full_long$lab_id == "b6c8ec"

table(full_long$lab_id, full_long$comp, useNA = "ifany") 

# deal with not sures 
table(full_long$comp, useNA = "ifany")

full_long$comp[full_long$comp == "Not Sure" & is.na(full_long$comp_type)] <- "No"
full_long$comp[full_long$comp == "Not Sure" & !is.na(full_long$comp_type)] <- "Yes"

full_long$comp <- droplevels(full_long$comp)
```

### Fix Lab ID to Consistent "Data Collection Location ID"

```{r}
full_long <- merge(full_long, 
                   lab_sheet[ , c("lab_code", "UN_Region", "Person")],
                   by.x = "lab_id", by.y = "lab_code", 
                   all.x = T)
full_long$Person[is.na(full_long$Person)] <- as.character(full_long$lab_id[is.na(full_long$Person)])

table(full_long$Person)
```

### Examine Coding for Covariates 

  - Examine gender labels
  - Recode education to number and examine ... fix issues with two labs not using the same education values.

```{r}
# better labels for graphs
full_long$vignette <- factor(full_long$vignette,
                             levels = c("D", "E", "G"),
                             labels = c("Darrel", "Emma", "Gerald"))

full_long$cond <- factor(full_long$cond,
                             levels = c("G", "I", "K"),
                             labels = c("Gettier", "Ignorance", "Knowledge"))

# gender
table(full_long$gender, useNA = "ifany")

# describe education years
full_long$education <- as.numeric(full_long$education)
describe(full_long$education[!duplicated(full_long$id)])

levels(full_long$education_level)

# less high school = 10  
# high school = 12
# some college = 14
# college = 16
# masters = 18
# graduate = 20

# levels(full_long$education) <- c(levels(full_long$education), 20, 21)
# truncate all education since that's how it was on socisurvey

full_long$education[full_long$education_level == "Less than high school" & !is.na(full_long$education_level)] <- 10
full_long$education[full_long$education_level == "High school diploma (or GED)" & !is.na(full_long$education_level)] <- 12
full_long$education[full_long$education_level == "Some college or a 2-year college degree (A.A.)" & !is.na(full_long$education_level)] <- 14
full_long$education[full_long$education_level == "4-year college degree (B.A., B.S.)" & !is.na(full_long$education_level)] <- 16
full_long$education[full_long$education_level == "Master’s degree (M.A., M.S.)" & !is.na(full_long$education_level)] <- 18 #> 17 group
full_long$education[full_long$education_level == "Graduate or professional degree (J.D., Ph.D., M.D.)" & !is.na(full_long$education_level)] <-  18 #>17 group

# PRT lab bach = 15, master = 17, PHD = 21
full_long$education[full_long$education_level == "4-year college degree (B.A., B.S.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <- 15
full_long$education[full_long$education_level == "Master’s degree (M.A., M.S.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <- 17
full_long$education[full_long$education_level == "Graduate or professional degree (J.D., Ph.D., M.D.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <-  18 #> 17 group 

# describe age
describe(full_long$age[!duplicated(full_long$id)])

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))
# number of data collection spot
length(unique(full_long$Person))
```

## Study Exclusions

In this section, we will mark each exclusion criteria to be able to denote how many exclusions each participant may have had.

### Majority Age

(1) if the participant is not the majority age of their country or older (unless parent/guardian waiver provided)

In this section, we will exclude all participants who are under 18 or are not the majority age of their country or older.

https://en.wikipedia.org/wiki/Age_of_majority

```{r}
# table of lab countries
table(full_long$lab_country, useNA = "ifany")

# age information
summary(full_long$age)

# minimum age by country
tapply(full_long$age, full_long$lab_country, min, na.rm = T)

# exclude based on age
full_long$age_exclusion <- NA

age18 <- c("AUT", "AUS", "CAN", "CHE", "DEU", "GBR", "GRC", "HUN", "NOR", "NZL", "POL", "PRT", "ROU", "RUS", "SGP", "TUR", "TWN", "USA")
age19 <- c("SVK")
age21 <- c("ARE")

full_long$age_exclusion[full_long$lab_country %in% age18] <- full_long$age[full_long$lab_country %in% age18] < 18

full_long$age_exclusion[full_long$lab_country %in% age19] <- full_long$age[full_long$lab_country %in% age19] < 19

full_long$age_exclusion[full_long$lab_country %in% age21] <- full_long$age[full_long$lab_country %in% age21] < 21

full_long$age_exclusion[is.na(full_long$age_exclusion)] <- TRUE

full_long$age_exclusion[full_long$age > 100] <- TRUE
```

### Previous Knowledge/Participation of Study 

(2) if the participant has taken part in a previous version of this study or in another contributors' replication of the same study

```{r}
full_long <- merge(full_long,
                   previousDF_wide[ , c("id", "exclude", "nonsense")], all.x = T) 

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))

colnames(full_long)[(ncol(full_long)-1):ncol(full_long)] <- c("previous_exclusion", "previous_nonsense")
```

### Comprehension Questions 

(3) if the participant fails to answer comprehension questions correctly

```{r}
table(full_long$dge_valid)

full_long$studyans_exclusion <- !(full_long$dge_valid)
```

### Knowledge of Experiment/Hypothesis 

(4) if the participant correctly and explicitly articulate knowledge of the specific hypotheses or specific conditions of this study when answering the funneled debriefing questions. 

```{r}
full_long <- merge(full_long,
                   purposeDF_wide[ , c("id", "exclude", "nonsense")], all.x = T) 

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))

colnames(full_long)[(ncol(full_long)-1):ncol(full_long)] <- c("purpose_exclusion", "purpose_nonsense")
```

### Language Proficiency 

We will also exclude participants who self-report their understanding of the tested language as "not well" or "not well at all". We based this exclusion criteria on a recent study that found that non-native English speakers who self-report as "very well" and "well" tend to score in the "intermediate" and "basic" categories on an English proficiency test respectively, while those who self-report as "not well" and "not at all" tend to score in the "below basic" category (Vickstrom, Shin, Collazo, & Bauman, 2015). All excluded data will be included in the data files on the overall OSF page, along with the particular reason for why they were excluded. 

```{r}
summary(full_long$language)

full_long$lang_exclusion <- full_long$language == "not very well" | full_long$language == "not well at all" 

full_long$lang_exclusion[is.na(full_long$lang_exclusion)] <- TRUE
```

### Total Exclusions

In this section, we provide the total (data point) exclusions for each particular screening item plus a table with all exclusions forever. 

```{r}
full_long$total_exclusion <- apply(full_long[ , grep("exclusion", colnames(full_long))], 1, sum)

table(full_long$age_exclusion)
table(full_long$lang_exclusion)
table(full_long$studyans_exclusion)
table(full_long$previous_exclusion)
table(full_long$purpose_exclusion)
table(full_long$total_exclusion)

# multiway table of all exclusions 
exclusions <- as.data.frame(table(full_long$age_exclusion, 
                                  full_long$lang_exclusion, 
                                  full_long$previous_exclusion,
                                  full_long$purpose_exclusion, 
                                  full_long$studyans_exclusion))
colnames(exclusions) <- c("Age", "Language", "Previous", "Purpose", "Comprehension", "Frequency")
exclusions$Num_Participants <- exclusions$Frequency / 3

exclusions

final_long <- subset(full_long, total_exclusion < 1)

# total number data points originally
nrow(full_long) 
# total number data points now
nrow(final_long) 

# original number of participants 
length(unique(full_long$id)) 
# final number of participants 
length(unique(final_long$id)) 
```

## Examine the Distributions

```{r}
table(final_long$bin_order)
final_long$bin_order <- droplevels(final_long$bin_order)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")
```

## Descriptives and Demograhics

```{r}
# total lab count
length(unique(full_long$Person))

# usable data lab count
length(unique(final_long$Person))

# total countries
table(full_long$lab_country)
length(table(full_long$lab_country))

# usable countries
table(final_long$lab_country)
length(table(final_long$lab_country))

# proportion usable data by country
table(final_long$lab_country) / table(full_long$lab_country)
```

## Analysis Overview 

- Analysis:
  - Multilevel modeling using appropriate linking function 
- Dependent variable(s):
  - Knowledge
  - Luck
  - Reasonable
- Independent variable:
  - Condition 
- Covariates: 
  - Test setting: compensated versus uncompensated
  - Age
  - Gender (men/women/other)
  - Years of education 
- Random intercepts:
  - participant / vignette / lab / maybe geographic region
- Special population:
  - Turk data on similar analysis 
  
## Data Screening

In this section, we will screen the continuous VAS variable, as the BIN variable does not require the same screening because of the different assumptions. 

### Knowledge 

```{r}
# drop levels
final_long$id <- factor(final_long$id)
final_long$vignette <- factor(final_long$vignette)

# check numbers
table(final_long$cond, useNA = "ifany")
table(final_long$comp, useNA = "ifany")
table(final_long$gender, useNA = "ifany")
length(na.omit(final_long$know_vas))

# check data
hist(final_long$know_vas)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")

## knowledge 
ds.model <- lme(know_vas ~ cond + comp + age + gender + education,
                random = list(~1|vignette, ~1|id, ~1|Person),
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 
# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Reasonableness 

```{r}
# check data
hist(final_long$reason_vas)

# visualize the problem 
ggplot(final_long, aes(reason_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Reasonableness Attribution Visual Analogue Score")

## reasonable 
ds.model <- lme(reason_vas ~ cond + comp_type + age + 
                  gender + education,
                random = list(~1|vignette, ~1|id), #won't converge with Person
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 

# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Luck 

```{r}
# check data
hist(final_long$luck_vas)

# visualize the problem 
ggplot(final_long, aes(luck_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Luck Attribution Visual Analogue Score")

ds.model <- lme(luck_vas ~ cond + comp_type + age + 
                  gender + education ,
                random = list(~1|vignette, ~1|id),
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 

# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Update DV  

In this instance, we do not see any form of linearity or homoscedasticity - the data is clearly skewed or bimodal. Therefore, we decided to split the VAS scale into 40 and below and 60 and above, then combine with the truly binary data, and include this variable to understand if there were differences when the VAS scale did not produce continuous data. 

Please note that the VAS is scored in the opposite direction of the BIN scales, therefore, you will see that the data split corrects this directionality difference. 

```{r}
## knowledge
table(final_long$know_bin)

## split data 
final_long$know_vas_binned <- final_long$know_vas
final_long$know_vas_binned[final_long$know_vas_binned <= 40] <- 2
final_long$know_vas_binned[final_long$know_vas_binned >= 40 & 
                             final_long$know_vas_binned <= 60] <- NA
final_long$know_vas_binned[final_long$know_vas_binned >= 60] <- 1
table(final_long$know_vas_binned, useNA = "ifany")

# no overlap
table(final_long$know_bin, final_long$know_vas_binned, useNA = "ifany")

final_long$know_vas_combined <- ifelse(is.na(final_long$know_vas_binned), 
                                     final_long$know_bin, 
                                     final_long$know_vas_binned)

final_long$know_vas_combined_source <- ifelse(is.na(final_long$know_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$luck_vas_combined)

## reasonable
table(final_long$reason_bin)

## split data 
final_long$reason_vas_binned <- final_long$reason_vas
final_long$reason_vas_binned[final_long$reason_vas_binned <= 40] <- 2
final_long$reason_vas_binned[final_long$reason_vas_binned >= 40 & 
                             final_long$reason_vas_binned <= 60] <- NA
final_long$reason_vas_binned[final_long$reason_vas_binned >= 60] <- 1
table(final_long$reason_vas_binned, useNA = "ifany")

# no overlap
table(final_long$reason_bin, final_long$reason_vas_binned, useNA = "ifany")

final_long$reason_vas_combined <- ifelse(is.na(final_long$reason_vas_binned), 
                                     final_long$reason_bin, 
                                     final_long$reason_vas_binned)

final_long$reason_vas_combined_source <- ifelse(is.na(final_long$reason_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$reason_vas_combined)
table(final_long$reason_vas_combined_source)

## luck
table(final_long$luck_bin)

## split data 
final_long$luck_vas_binned <- final_long$luck_vas
final_long$luck_vas_binned[final_long$luck_vas_binned <= 40] <- 2
final_long$luck_vas_binned[final_long$luck_vas_binned >= 40 & 
                             final_long$luck_vas_binned <= 60] <- NA
final_long$luck_vas_binned[final_long$luck_vas_binned >= 60] <- 1
table(final_long$luck_vas_binned, useNA = "ifany")

# no overlap
table(final_long$luck_bin, final_long$luck_vas_binned, useNA = "ifany")

final_long$luck_vas_combined <- ifelse(is.na(final_long$luck_vas_binned), 
                                     final_long$luck_bin, 
                                     final_long$luck_vas_binned)

final_long$luck_vas_combined_source <- ifelse(is.na(final_long$luck_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$luck_vas_combined)
```

### Gender Other

Because of the small other category, we will simply compare male to female. 

```{r}
final_long$gender2 <- factor(final_long$gender,
                             levels = c("female", "male"))
```

## Data Checks

### Missing Data Information 

```{r}
table(final_long$Person, final_long$vignette, useNA = "ifany")

table(final_long$Person, final_long$comp, useNA = "ifany") # 6ce754 and f6d5a9

table(final_long$Person, final_long$gender2, useNA = "ifany")

table(final_long$Person, final_long$cond, useNA = "ifany")

table(final_long$Person, final_long$know_vas_combined, useNA = "ifany")

table(final_long$Person, final_long$reason_vas_combined, useNA = "ifany")

table(final_long$Person, final_long$luck_vas_combined, useNA = "ifany")

table(final_long$Person, final_long$education)

tapply(final_long$age, final_long$Person, mean, na.rm = T)
```

### Check for Reverse Coding 

```{r}
table(final_long$bin_order)
final_long$bin_order <- droplevels(final_long$bin_order)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")

# visualize the problem 
ggplot(final_long, aes(know_bin)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Bin Score")
```

 ## Final Analysis Knowledge

### Model 1 

Intercept only model to help determine if the addition of random intercepts are useful.

```{r}
#scores should be 0 and 1 for this analysis 
final_long$know_vas_combined <- final_long$know_vas_combined - 1

k.model.1 <- glm(know_vas_combined ~ 1,
               data = final_long,
               family = binomial,
               na.action = "na.omit")

summary(k.model.1)
```

### Model 2

Vignette random intercept model:

```{r}
k.model.2 <- glmer(know_vas_combined ~ (1|vignette),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.2)
coef(k.model.2) # log odds zero is the center 
exp(coef(k.model.2)$vignette) # regular odds with 1 at the center 
table(final_long$vignette, final_long$know_vas_combined)
r.squaredGLMM(k.model.2)
```

  - Remember that 0 is knows and 1 is believes 
  - Model 1 AIC: `r AIC(k.model.1)`
  - Model 2 AIC: `r AIC(k.model.2)`
  - Interpretation: the inclusion of the vignette random variable is useful
  - Coefficients: interpretation overall of random intercept, which effectively tells us regardless of condition, which way the participants lean toward answering. See the table printed out to help understand these values. 
    - Darrel: fairly even split for knows/believes with a slight favor to knows (because it's negative goes toward the 0 group)
    - Emma: leans more heavily toward the believes category (almost 4 to 1 odds)
    - Gerald: leans slightly toward the believes category
    - The results here indicate what we were seeing with the "continuous" data as well - that there's a big difference in the way they respond to vignettes
  - This vignette accounts for 8 to 10 percent of the random variance. 
  
### Model 3

Vignette nested in participants random intercept:

```{r}
k.model.3 <- glmer(know_vas_combined ~ (1|vignette/id),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.3)
r.squaredGLMM(k.model.3)
```


  - Model 2 AIC: `r AIC(k.model.2)`
  - Model 3 AIC: `r AIC(k.model.3)`
  - Interpretation: the inclusion of participant ID isn't really useful, as the AIC is higher for this model. The variance between participants is very small after accounting for vignettes.
  - This model accounts for approximately the same random variance. 
  
### Model 4

Vignette nested in participants which is then nested in labs:

```{r}
k.model.4 <- glmer(know_vas_combined ~ (1|vignette/id/Person),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.4)
r.squaredGLMM(k.model.4)
```

  - Model 2 AIC: `r AIC(k.model.2)`
  - Model 3 AIC: `r AIC(k.model.2)`
  - Model 4 AIC: `r AIC(k.model.4)`
  - Interpretation: the inclusion of lab ID isn't really useful, as the AIC is higher for this model. The variance parameter for lab is also very small. 
  - This model accounts for approximately the same random variance.
  - We will continue to use this cross classified structure to ensure we account for correlated error and to follow the pre-print.
  
### Model 5

Addition of the covariates added to the vignette random intercept model. 

```{r}
k.model.5 <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.5)

r.squaredGLMM(k.model.5)
```

  - Model 4 AIC: `r AIC(k.model.4)`
  - Model 5 AIC: `r AIC(k.model.5)`
  - Interpretation: this model is better than the vignette random intercept only model.
  - No covariates predicted the outcome variable. 
  
### Model 6

The previous model of covariates plus the variable of focus: condition.

```{r}
k.model.6 <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6)

table(k.model.6@frame$cond, k.model.6@frame$know_vas_combined)

r.squaredGLMM(k.model.6)
```

  - Model 5 AIC: `r AIC(k.model.5)`
  - Model 6 AIC: `r AIC(k.model.6)`
  - Interpretation: this model is better than the model with the covariates. 
  - Both knowledge and ignorance are different than the Gettier condition.
  - In Gettier, participants are slightly more likely to say believes.
  - In Ignorance, participants are *way* more likely to say believes (so that's why this condition is different from Gettier).
  - In Knowledge, participants are more like to say knows (also why this is different from Gettier). 
  - This effect accounts for 13-15% of the fixed effect variance. 

### Model 6A

Does the vignette interact with the condition?

```{r}
k.model.6A <- glmer(know_vas_combined ~ (1 | vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond*vignette,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6A)

r.squaredGLMM(k.model.6A)

data_graph <- k.model.6A@frame
data_graph$know_vas_combined <- factor(data_graph$know_vas_combined,
                                          levels = c(0,1),
                                          labels = c("Knows", "Believes"))

#graph the three way binary 
ggplot(data_graph) +
  geom_mosaic(aes(x = product(know_vas_combined, cond, vignette), 
                  fill = know_vas_combined)) + 
  scale_fill_manual(name = "Knowledge Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.5,.87),
    labels = c("Darrel", "Emma", "Gerald")) + 
  theme_classic() + 
  xlab("Vignette") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6A AIC: `r AIC(k.model.6A)`
  - Interpretation: This model is better than a model without the interaction. 
  - There is an interaction between vignette and condition for choice. 
  - In looking at the graph, we can see this is driven by the Emma vignette which shows a larger believes box for all conditions. 
  - This interaction adds a good portion of variance to the fixed effects 7-9%. 

To analyze this interaction, we will use chi-square analysis and proportion tests (making sure it's on the same data as the analysis above).

```{r}
# overall darrel
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Darrel"], 
           data_graph$cond[data_graph$vignette == "Darrel"])
d_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Darrel"], 
           data_graph$cond[data_graph$vignette == "Darrel"])
d_table

# prop tests
prop1 <- prop.test(t(d_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(d_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(d_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(d_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- The proportions are the likelihood of Knows for each condition
- Knows more likely in Gettier than Ignorance 
- Knows more likely in Knowledge than Gettier  

```{r}
# overall emma
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Emma"], 
           data_graph$cond[data_graph$vignette == "Emma"])
e_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Emma"], 
           data_graph$cond[data_graph$vignette == "Emma"])
e_table

# prop tests
prop1 <- prop.test(t(e_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(e_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(e_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(e_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- Knows more likely in Gettier than Ignorance 
- Knows more likely in Knowledge than Gettier  

```{r}
# overall gerald
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Gerald"], 
           data_graph$cond[data_graph$vignette == "Gerald"])
g_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Gerald"], 
           data_graph$cond[data_graph$vignette == "Gerald"])
g_table

# prop tests
prop1 <- prop.test(t(g_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(g_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(g_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(g_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- Knows more likely in Gettier than Ignorance 
- Knows more likely in Knowledge than Gettier
- While all three of these show the same results, we see that their strengths are different for each one using Cramer's V. 

### Model 6B

Data was collected from mechanical turk participants. Are there differences in this data and the complete data on the condition variable? 

```{r}
k.model.6B <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*turk,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6B)

r.squaredGLMM(k.model.6B)
```

  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6A AIC: `r AIC(k.model.6B)`
  - Interpretation: This model is better than a model without the interaction. 
  - However, the interaction with the turk variable does not produce significant effects.
  - The model is likely "better" because of the added variable and main effect of turk (likely drive by sample size differences in the two samples of turk versus not for knows versus believes). 
  - Therefore, we do not find evidence for differences in the turk sample for condition effects. 

### Model 6C

Data was binned because of the distributions of the visual analogue scale - does the data source make a difference? 

```{r}
k.model.6C <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*know_vas_combined_source,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6C)

r.squaredGLMM(k.model.6C)
```

  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6C AIC: `r AIC(k.model.6C)`
  - Interpretation: This model is not better than a model without the condition interaction. 
  - There is no evidence for an interaction between condition and the data source, indicating that the binning process did not give us evidence of different results. 
  
## Final Analysis Reasonableness

### Model 1 

Intercept only model to help determine if the addition of random intercepts are useful.

```{r}
#scores should be 0 and 1 for this analysis 
final_long$reason_vas_combined <- final_long$reason_vas_combined - 1

r.model.1 <- glm(reason_vas_combined ~ 1,
               data = final_long,
               family = binomial,
               na.action = "na.omit")

summary(r.model.1)
```

### Model 2

Vignette random intercept model:

```{r}
r.model.2 <- glmer(reason_vas_combined ~ (1|vignette),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.2)
coef(r.model.2) # log odds zero is the center 
exp(coef(r.model.2)$vignette) # regular odds with 1 at the center 
table(final_long$vignette, final_long$reason_vas_combined)
r.squaredGLMM(r.model.2)
```

  - Remember that 0 is reasonable, 1 is unreasonable 
  - Model 1 AIC: `r AIC(r.model.1)`
  - Model 2 AIC: `r AIC(r.model.2)`
  - Interpretation: the inclusion of the vignette random variable is useful
  - Coefficients: interpretation overall of random intercept, which effectively tells us regardless of condition, which way the participants lean toward answering. See the table printed out to help understand these values. 
    - We see that they all lean towards reasonable at slightly different strengths. 
    - The intercept accounts for only a small amount of random variance, < .01 to .02. 
  
### Model 3

Vignette nested in participants random intercept:

```{r}
r.model.3 <- glmer(reason_vas_combined ~ (1|vignette/id),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.3)
r.squaredGLMM(r.model.3)
```


  - Model 2 AIC: `r AIC(r.model.2)`
  - Model 3 AIC: `r AIC(r.model.3)`
  - Interpretation: the inclusion of participant ID isn't really useful, as the AIC is higher for this model. The variance between participants is very small after accounting for vignettes.
  - This model accounts for approximately the same random variance. 
  
### Model 4

Vignette nested in participants which is then nested in labs:

```{r}
r.model.4 <- glmer(reason_vas_combined ~ (1|vignette/id/Person),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.4)
r.squaredGLMM(r.model.4)
```

  - Model 2 AIC: `r AIC(r.model.2)`
  - Model 3 AIC: `r AIC(r.model.3)`
  - Model 4 AIC: `r AIC(r.model.4)`
  - Interpretation: the inclusion of lab ID isn't really useful, as the AIC is higher for this model. The variance parameter for lab is also very small. 
  - This model accounts for approximately the same random variance.
  - We will continue to use this cross classified structure to ensure we account for correlated error and to follow the pre-print.
  
### Model 5

Addition of the covariates added to the vignette random intercept model. 

```{r}
r.model.5 <- glmer(reason_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.5)

table(r.model.5@frame$comp, r.model.5@frame$reason_vas_combined)

r.squaredGLMM(r.model.5)
```

  - Model 4 AIC: `r AIC(r.model.4)`
  - Model 5 AIC: `r AIC(r.model.5)`
  - Interpretation: this model is better than the vignette random intercept only model.
  - Coefficients:
    - The compensated participants were more likely to say reasonable (they are also a much larger group).
    - Education is a negative coefficient indicating that the less educate a person listed, the more likely they were to pick reasonable. 
  
### Model 6

The previous model of covariates plus the variable of focus: condition.

```{r}
r.model.6 <- glmer(reason_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.6)

table(r.model.6@frame$cond, model.6@frame$reason_vas_combined)

r.squaredGLMM(r.model.6)
```

  - Model 5 AIC: `r AIC(r.model.5)`
  - Model 6 AIC: `r AIC(r.model.6)`
  - Interpretation: this model is better than the model with the covariates. 
  - Both knowledge and ignorance are different than the Gettier condition.
  - In Ignorance, participants are likely to say reasonable ... but in Gettier, they are more likely to say reasonable.
  - In Knowledge, participants are more like to say reasonable than the Gettier condition.
  - This effect accounts for 1-4% of the fixed effect variance. 

### Model 6A

Does the vignette interact with the condition?

```{r}
r.model.6A <- glmer(reason_vas_combined ~ (1 | vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond*vignette,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.6A)

r.squaredGLMM(r.model.6A)

r.data_graph <- model.6A@frame
r.data_graph$reason_vas_combined <- factor(r.data_graph$reason_vas_combined,
                                          levels = c(0,1),
                                          labels = c("Reasonable", "Unreasonable"))

#graph the three way binary 
ggplot(r.data_graph) +
  geom_mosaic(aes(x = product(reason_vas_combined, cond, vignette), 
                  fill = reason_vas_combined)) + 
  scale_fill_manual(name = "Reasonable Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.5,.87),
    labels = c("Darrel", "Emma", "Gerald")) + 
  theme_classic() + 
  xlab("Vignette") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(r.model.6)`
  - Model 6A AIC: `r AIC(r.model.6A)`
  - Interpretation: This model is better than a model without the interaction. 
  - There is an interaction between vignette and condition for choice. 
  - In looking at the graph, we can see this is driven by the Emma vignette which shows a larger unreasonable box for all conditions. 
  - This interaction adds a portion of variance to the fixed effects 1-5%. 

To analyze this interaction, we will use chi-square analysis and proportion tests (making sure it's on the same data as the analysis above).

```{r}
# overall darrel
chisq.test(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Darrel"], 
           r.data_graph$cond[data_graph$vignette == "Darrel"])
d_table <- table(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Darrel"], 
           r.data_graph$cond[r.data_graph$vignette == "Darrel"])
d_table

# prop tests
prop1 <- prop.test(t(d_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(d_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(d_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(d_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- The proportions are the likelihood of Reasonable for each condition
- Reasonable more likely in Gettier than Ignorance 
- No differences for reasonable in the Gettier and Knowledge Condition

```{r}
# overall emma
chisq.test(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Emma"], 
           r.data_graph$cond[r.data_graph$vignette == "Emma"])
e_table <- table(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Emma"], 
           r.data_graph$cond[r.data_graph$vignette == "Emma"])
e_table

# prop tests
prop1 <- prop.test(t(e_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(e_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(e_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(e_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- No differences in Gettier and Ignorance 
- Reasonable more likely in the Knowledge Condition than Gettier 

```{r}
# overall gerald
chisq.test(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Gerald"], 
           r.data_graph$cond[r.data_graph$vignette == "Gerald"])
g_table <- table(r.data_graph$reason_vas_combined[r.data_graph$vignette == "Gerald"], 
           r.data_graph$cond[r.data_graph$vignette == "Gerald"])
g_table

# prop tests
prop1 <- prop.test(t(g_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(g_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(g_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(g_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- Reasonable more likely in the Gettier than the Ignorance
- No differences in the Geitter and the Knowledge conditions 

### Model 6B

Data was collected from mechanical turk participants. Are there differences in this data and the complete data on the condition variable? 

```{r}
r.model.6B <- glmer(reason_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*turk,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.6B)

r.squaredGLMM(r.model.6B)
```

  - Model 6 AIC: `r AIC(r.model.6)`
  - Model 6A AIC: `r AIC(r.model.6B)`
  - Interpretation: This model is better than a model without the interaction. 
  - However, the interaction with the turk variable does not produce significant effects.
  - The model is likely "better" because of the added variable and main effect of turk (likely drive by sample size differences in the two samples of turk versus not for knows versus believes). 
  - Therefore, we do not find evidence for differences in the turk sample for condition effects. 

### Model 6C

Data was binned because of the distributions of the visual analogue scale - does the data source make a difference? 

```{r}
r.model.6C <- glmer(reason_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*reason_vas_combined_source,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(r.model.6C)

r.squaredGLMM(r.model.6C)
```

  - Model 6 AIC: `r AIC(r.model.6)`
  - Model 6C AIC: `r AIC(r.model.6C)`
  - Interpretation: This model is better than a model without the source interaction. 
  - There is no evidence for an interaction between condition and the data source, indicating that the binning process did not give us evidence of different results. 

## Final Analysis Luck

### Model 1 

Intercept only model to help determine if the addition of random intercepts are useful.

```{r}
#scores should be 0 and 1 for this analysis 
final_long$luck_vas_combined <- final_long$luck_vas_combined - 1

l.model.1 <- glm(luck_vas_combined ~ 1,
               data = final_long,
               family = binomial,
               na.action = "na.omit")

summary(l.model.1)
```

### Model 2

Vignette random intercept model:

```{r}
l.model.2 <- glmer(luck_vas_combined ~ (1|vignette),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.2)
coef(l.model.2) # log odds zero is the center 
exp(coef(l.model.2)$vignette) # regular odds with 1 at the center 
table(final_long$vignette, final_long$luck_vas_combined)
r.squaredGLMM(l.model.2)
```

  - Remember that 0 is luck and 1 is ability 
  - Model 1 AIC: `r AIC(l.model.1)`
  - Model 2 AIC: `r AIC(l.model.2)`
  - Interpretation: the inclusion of the vignette random variable is useful
  - Coefficients: interpretation overall of random intercept, which effectively tells us regardless of condition, which way the participants lean toward answering. See the table printed out to help understand these values. 
    - Darrel: People are more likely to pick ability 
    - Emma: People are more likely to pick luck
    - Gerald: lPeople are more even, but lean toward luck 
    - The results here indicate what we were seeing with the "continuous" data as well - that there's a big difference in the way they respond to vignettes
  - This vignette accounts for 8 to 10 percent of the random variance. 
  
### Model 3

Vignette nested in participants random intercept:

```{r}
l.model.2 <- glmer(luck_vas_combined ~ (1|vignette/id),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.2)
r.squaredGLMM(l.model.2)
```


  - Model 2 AIC: `r AIC(l.model.2)`
  - Model 3 AIC: `r AIC(l.model.2)`
  - Interpretation: the inclusion of participant ID isn't really useful, as the AIC is higher for this model. The variance between participants is very small after accounting for vignettes.
  - This model accounts for approximately the same random variance. 
  
### Model 4

Vignette nested in participants which is then nested in labs:

```{r}
l.model.4 <- glmer(luck_vas_combined ~ (1|vignette/id/Person),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.4)
r.squaredGLMM(l.model.4)
```

  - Model 2 AIC: `r AIC(l.model.2)`
  - Model 3 AIC: `r AIC(l.model.3)`
  - Model 4 AIC: `r AIC(l.model.4)`
  - Interpretation: the inclusion of lab ID isn't really useful, as the AIC is higher for this model. The variance parameter for lab is also very small. 
  - This model accounts for approximately the same random variance.
  - We will continue to use this cross classified structure to ensure we account for correlated error and to follow the pre-print.
  
### Model 5

Addition of the covariates added to the vignette random intercept model. 

```{r}
l.model.5 <- glmer(luck_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.5)

table(l.model.5@frame$luck_vas_combined, l.model.5@frame$comp)

r.squaredGLMM(l.model.5)
```

  - Model 4 AIC: `r AIC(l.model.4)`
  - Model 5 AIC: `r AIC(l.model.5)`
  - Interpretation: this model is better than the vignette random intercept only model.
  - Coeffients:
    - Education: this effect indicates that people who select lower education levels are more likely to select luck versus ability. The variance accounted for is small. 
    
### Model 6

The previous model of covariates plus the variable of focus: condition.

```{r}
l.model.6 <- glmer(luck_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.6)

table(l.model.6@frame$cond, l.model.6@frame$luck_vas_combined)

r.squaredGLMM(l.model.6)
```

  - Model 5 AIC: `r AIC(model.5)`
  - Model 6 AIC: `r AIC(model.6)`
  - Interpretation: this model is better than the model with the covariates. 
  - Both knowledge and ignorance are different than the Gettier condition.
  - In Gettier, people are more likely to chose luck.
  - In Ignorance, people are more likely to chose ability.
  - In Knowledge, people are more likely to chose ability. 
  - This effect accounts for 4-5% of the fixed effect variance. 

### Model 6A

Does the vignette interact with the condition?

```{r}
model.6A <- glmer(luck_vas_combined ~ (1 | vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond*vignette,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.6A)

r.squaredGLMM(model.6A)

l.data_graph <- model.6A@frame
l.data_graph$luck_vas_combined <- factor(l.data_graph$luck_vas_combined,
                                          levels = c(0,1),
                                          labels = c("Luck", "Ability"))

#graph the three way binary 
ggplot(l.data_graph) +
  geom_mosaic(aes(x = product(luck_vas_combined, cond, vignette), 
                  fill = luck_vas_combined)) + 
  scale_fill_manual(name = "Luck Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.5,.87),
    labels = c("Darrel", "Emma", "Gerald")) + 
  theme_classic() + 
  xlab("Vignette") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(model.6)`
  - Model 6A AIC: `r AIC(model.6A)`
  - Interpretation: This model is better than a model without the interaction. 
  - There is an interaction between vignette and condition for choice. 
  - In looking at the graph, we can see that each vignette produces a different pattern of results. 
  - This interaction adds a good portion of variance to the fixed effects 14-17%. 

To analyze this interaction, we will use chi-square analysis and proportion tests (making sure it's on the same data as the analysis above).

```{r}
# overall darrel
chisq.test(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Darrel"], 
           l.data_graph$cond[l.data_graph$vignette == "Darrel"])
d_table <- table(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Darrel"], 
           l.data_graph$cond[l.data_graph$vignette == "Darrel"])
d_table

# prop tests
prop1 <- prop.test(t(d_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(d_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(d_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(d_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- The proportions are the likelihood of Luck for each condition
- No differences in Gettier and Ignorance 
- More likely to pick luck in Gettier than Knowledge 

```{r}
# overall emma
chisq.test(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Emma"], 
           l.data_graph$cond[l.data_graph$vignette == "Emma"])
e_table <- table(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Emma"], 
           l.data_graph$cond[l.data_graph$vignette == "Emma"])
e_table

# prop tests
prop1 <- prop.test(t(e_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(e_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(e_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(e_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- More likely to pick luck in Gettier than Ignorance
- More likely to pick luck in Gettier than Knowledge

```{r}
# overall gerald
chisq.test(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Gerald"], 
           l.data_graph$cond[l.data_graph$vignette == "Gerald"])
g_table <- table(l.data_graph$luck_vas_combined[l.data_graph$vignette == "Gerald"], 
           l.data_graph$cond[l.data_graph$vignette == "Gerald"])
g_table

# prop tests
prop1 <- prop.test(t(g_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(g_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(g_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(g_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- More likely to pick luck in Gettier than Ignorance
- More likely to pick luck in Gettier than Knowledge

### Model 6B

Data was collected from mechanical turk participants. Are there differences in this data and the complete data on the condition variable? 

```{r}
l.model.6B <- glmer(luck_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*turk,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.6B)

r.squaredGLMM(l.model.6B)
```

  - Model 6 AIC: `r AIC(l.model.6)`
  - Model 6A AIC: `r AIC(l.model.6B)`
  - Interpretation: This model is better than a model without the interaction. 
  - However, the interaction with the turk variable does not produce significant effects.
  - The model is likely "better" because of the added variable and main effect of turk (likely drive by sample size differences in the two samples of turk versus not for knows versus believes). 
  - Therefore, we do not find evidence for differences in the turk sample for condition effects. 

### Model 6C

Data was binned because of the distributions of the visual analogue scale - does the data source make a difference? 

```{r}
l.model.6C <- glmer(luck_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*luck_vas_combined_source,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(l.model.6C)

table(l.model.6C@frame$luck_vas_combined, l.model.6C@frame$cond, l.model.6C@frame$luck_vas_combined_source)

r.squaredGLMM(l.model.6C)

l.data_graph <- l.model.6C@frame
l.data_graph$luck_vas_combined <- factor(l.data_graph$luck_vas_combined,
                                          levels = c(0,1),
                                          labels = c("Luck", "Ability"))

#graph the three way binary 
ggplot(l.data_graph) +
  geom_mosaic(aes(x = product(luck_vas_combined, cond, luck_vas_combined_source), 
                  fill = luck_vas_combined)) + 
  scale_fill_manual(name = "Luck Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.87),
    labels = c("Binary", "VAS")) + 
  theme_classic() + 
  xlab("Source") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(l.model.6)`
  - Model 6C AIC: `r AIC(l.model.6C)`
  - Interpretation: This model is better than a model without the condition interaction. 
  - We do see evidence for some interaction with the source of the data. 

```{r}
# overall binary
chisq.test(l.data_graph$luck_vas_combined[l.data_graph$luck_vas_combined_source == "Binary"],
           l.data_graph$cond[l.data_graph$luck_vas_combined_source == "Binary"])

b_table <- table(l.data_graph$luck_vas_combined[l.data_graph$luck_vas_combined_source == "Binary"],
           l.data_graph$cond[l.data_graph$luck_vas_combined_source == "Binary"])

b_table

# prop tests
prop1 <- prop.test(t(b_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(b_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(b_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(b_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- Gettier more likely to say Luck than Ignorance
- Gettier more likely to say Luck than Knowledge 

```{r}
# overall VAS
chisq.test(l.data_graph$luck_vas_combined[l.data_graph$luck_vas_combined_source == "VAS"],
           l.data_graph$cond[l.data_graph$luck_vas_combined_source == "VAS"])

v_table <- table(l.data_graph$luck_vas_combined[l.data_graph$luck_vas_combined_source == "VAS"],
           l.data_graph$cond[l.data_graph$luck_vas_combined_source == "VAS"])

v_table

# prop tests
prop1 <- prop.test(t(v_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(v_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(v_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(v_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- Gettier more likely to say Luck than Ignorance
- Gettier more likely to say Luck than Knowledge 
- Interaction is likely driven by the differences in sample size and effect size for these effects (slightly stronger effects in the Binary condition but these overlap in their CI). 

## Exploratory Analyses

### Condition by Luck 

```{r}
k.model.6E <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                    comp + age + gender2 + education + 
                    cond*luck_vas_combined,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6E)

r.squaredGLMM(k.model.6E)

l2.data_graph <- k.model.6E@frame
l2.data_graph$know_vas_combined <- factor(l2.data_graph$know_vas_combined, 
                                       levels = 0:1,
                                       labels = c("Know", "Believes"))
l2.data_graph$luck_vas_combined <- factor(l2.data_graph$luck_vas_combined, 
                                       levels = 0:1,
                                       labels = c("Luck", "Ability"))

table(l2.data_graph$know_vas_combined, l2.data_graph$cond, l2.data_graph$luck_vas_combined)

#graph the three way binary 
ggplot(l2.data_graph) +
  geom_mosaic(aes(x = product(know_vas_combined, cond, luck_vas_combined), 
                  fill = know_vas_combined)) + 
  scale_fill_manual(name = "Knows Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.87),
    labels = c("Luck", "Ability")) + 
  theme_classic() + 
  xlab("Source") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6E AIC: `r AIC(k.model.6E)`
  - Interpretation: This model is better than the original model without the interaction.
  - There is an interaction between condition, luck choice to predict knowledge. 
  
```{r}
# break down by luck and ability 

# luck
l_table <- table(l2.data_graph$know_vas_combined[l2.data_graph$luck_vas_combined == "Luck"], l2.data_graph$cond[l2.data_graph$luck_vas_combined == "Luck"])
chisq.test(l2.data_graph$know_vas_combined[l2.data_graph$luck_vas_combined == "Luck"], l2.data_graph$cond[l2.data_graph$luck_vas_combined == "Luck"])

# prop tests
prop1 <- prop.test(t(l_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(l_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(l_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(l_table[1:2, c(1,3)])),
         r = 2, c = 2)

# ability
a_table <- table(l2.data_graph$know_vas_combined[l2.data_graph$luck_vas_combined == "Ability"], l2.data_graph$cond[l2.data_graph$luck_vas_combined == "Ability"])
chisq.test(l2.data_graph$know_vas_combined[l2.data_graph$luck_vas_combined == "Ability"], l2.data_graph$cond[l2.data_graph$luck_vas_combined == "Ability"])

# prop tests
prop1 <- prop.test(t(a_table[1:2, 1:2]))
prop1
v.chi.sq(x2 = prop1$statistic, 
         n = sum(t(a_table[1:2, 1:2])),
         r = 2, c = 2)

prop2 <- prop.test(t(a_table[1:2, c(1,3)]))
prop2
v.chi.sq(x2 = prop2$statistic, 
         n = sum(t(a_table[1:2, c(1,3)])),
         r = 2, c = 2)
```

- When we pick luck:
  - Gettier more likely to pick knows than Ignorance
  - Knowledge more likely to pick knows than Gettier
- When we pick ability: 
  - Gettier more likely to pick knows than Ignorance (by a lot)
  - Knowledge more likely to pick knows than Gettier (by a little)

### Alternative Knowledge Question

```{r}
final_long$know_alt <- final_long$know_alt - 1 #center over zero 

# what is the comparison of the original to alternative
table("original" = final_long$know_vas_combined, 
      "alternative" = final_long$know_alt)

k.model.6F <- glmer(know_alt ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6F)

table(k.model.6F@frame$cond, k.model.6F@frame$know_alt)

# for comparison approximately
table(final_long$cond, final_long$know_vas_combined)

r.squaredGLMM(k.model.6F)
```

  - Remember, 0 = knows, 1 = only feels 
  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6F AIC: `r AIC(k.model.6F)`
  - Interpretation: this model is "better" than the original, in that the AIC is lower, indicating lower error in predicting the outcome. 
  - In the Gettier condition, people are more likely to choose only feels
  - In the Ignorance condition, people are more likely to choose only feels but way more than the Gettier condition (why this predictor is significant)
  - In the Knowledge condition, they are fairly equal at choosing knows versus feels, but which is this one is different than Gettier
  - These results appear to be the same pattern as the original question

### Order Effects 

```{r}
k.model.6G <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond + vignette_order + cond_order,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6G)

r.squaredGLMM(k.model.6G)
```

  - Model 6 AIC: `r AIC(k.model.6)`
  - Model 6F AIC: `r AIC(k.model.6G)`
  - Interpretation: this model is "better" than the original, in that the AIC is lower, indicating lower error in predicting the outcome. This result isn't too surprising because we've added two new predictors. 
  - There do appear to be order effects for both predictors. 

```{r}
data_graph <- model.6G@frame

data_graph$know_vas_combined <- factor(data_graph$know_vas_combined, 
                                       levels = c(0,1),
                                       labels = c("Knows", "Believes"))

table(data_graph$know_vas_combined, data_graph$vignette_order)

#graph the two way binary 
ggplot(data_graph) +
  geom_mosaic(aes(x = product(know_vas_combined, vignette_order))) + 
  theme_classic() + 
  xlab("Vignette Order") + 
  ylab("Knowledge Choice")

table(data_graph$know_vas_combined, data_graph$cond_order)

ggplot(data_graph) +
  geom_mosaic(aes(x = product(know_vas_combined, cond_order))) + 
  theme_classic() + 
  xlab("Condition Order") + 
  ylab("Condition")
```

- However, these plots indicate that there's likely not a lot of effect for order - only slightly differences in knows versus believes selection. 

```{r}
k.model.6H <- glmer(know_vas_combined ~ (1|vignette/id/Person) + 
                   comp + age + gender2 + education + 
                   cond*vignette_order + cond*cond_order,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(k.model.6H)

r.squaredGLMM(k.model.6H)
```

  - Model 6 AIC: `r AIC(k.model.6G)`
  - Model 6F AIC: `r AIC(k.model.6H)`
  - Interpretation: This model is not better than a model with just the order effects.
  - There is no interaction between condition and order effects. 
