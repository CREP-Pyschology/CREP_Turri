---
title: "ACREP Updated Analyses"
author: "erin buchanan"
date: "Last Updated `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Information

Multilab close replication of: Experiment 1 from Turri, J., Buckwalter, W., & Blouw, P. (2015). Knowledge and luck. *Psychonomic Bulletin and Review*, 22, 378-390.

[Data and registered protocols:](https://osf.io/n5b3w/)

[Codebook](https://docs.google.com/spreadsheets/d/1KjXqgfVgguHeDXVtlHHhJ9zsRGVDfPVPH4tbh75P46U/edit#gid=903093128)

[Preprint](https://psyarxiv.com/zeux9/)

## Libraries

```{r}
library(rio)
library(lme4)
library(lmerTest)
library(nlme)
library(MuMIn)
library(plyr)
library(psych)
library(tidyr)
library(ggplot2)
library(dplyr)
library(ggmosaic)
```

## Import the Data

The `full_long` dataset includes all participants in long format - wherein each trial of their study is on one row of the dataset. Our uploaded data also includes `full.Rds` which is the same data in wide format - one column for each variable in the dataset and one row per participant.

```{r}
full_long <- readRDS("../04_Data/rds/d_all_long.Rds")
#str(full_long)
```

Import the open-ended responses and prepare them for merging with the final dataset. This section:

  - Imports all the files
  - Includes a coder id
  - Corrects all the issues with typed codings
  - Converts each item to coder counts so you can convert from wide to long
  - Converts from wide to long
  - Converts into scoring:
    - No or NA gets 0 points 
    - Maybe gets 1 point
    - Yes/test gets 2 points
    - Participants are marked as "excluded" if they get 4 total points across three coders 
  - Participants are also marked as "nonsense" if they did not write a legible answer or simply typed gibberish. These data points are not excluded but marked if you would like to repeat the analysis and exclude them. 

```{r}
file.names <- list.files(path = "../04_Data/open_responses/", 
                         pattern = ".xlsx", full.names = T, 
                         recursive = T)

previous.files <- lapply(file.names, function(x){ import(x, sheet = 2)})
purpose.files <- lapply(file.names, function(x){ import(x, sheet = 3)})

# add coder ids 
coder.id <- unlist(lapply(strsplit(file.names, "psa004"), function(x) x[[2]]))

names(previous.files) <- coder.id
names(purpose.files) <- coder.id

previousDF <- do.call(rbind.fill, previous.files)
purposeDF <- do.call(rbind.fill, purpose.files)

previousDF$coder <- rep(names(previous.files), unlist(lapply(previous.files, nrow)))
purposeDF$coder <- rep(names(purpose.files), unlist(lapply(purpose.files, nrow)))

# ensure the text is normalized 
previousDF$previous <- gsub("\\s+", " ", previousDF$previous)
purposeDF$purpose <- gsub("\\s+", " ", purposeDF$purpose)
previousDF$previous <- gsub("\\s$", "", previousDF$previous)
purposeDF$purpose <- gsub("\\s$", "", purposeDF$purpose)

# can't figure out why these two are different
purposeDF$purpose[purposeDF$id == "s10313"] <- "I'm not sure."
purposeDF$purpose[purposeDF$id == "s10513"] <- "Recall, recognizing"

# fix the yes/no/maybe/NA
previousDF$code <- tolower(previousDF$code)
table(previousDF$code)
previousDF$code <- gsub("np", "no", previousDF$code)
previousDF$code <- gsub("n/a", "na", previousDF$code)
previousDF$code <- gsub("y$", "yes", previousDF$code)
table(previousDF$code)
previousDF <- subset(previousDF, !is.na(id))

purposeDF$code <- tolower(purposeDF$code)
table(purposeDF$code)
purposeDF$code <- gsub("mayb$|meybe", "maybe", purposeDF$code)
purposeDF$code <- gsub("mo", "no", purposeDF$code)
purposeDF$code <- gsub("n/a", "na", purposeDF$code)
table(purposeDF$code)
purposeDF <- subset(purposeDF, !is.na(id))

# add coder count
previous_count <- as.data.frame(table(previousDF$id))
previous_count <- previous_count[order(previous_count$Var1), ]
previousDF <- previousDF[order(previousDF$id), ]
previousDF$coder_num <- NA

for (i in 1:length(previous_count$Freq)){
  previousDF$coder_num[previousDF$id == previous_count$Var1[i]] <- 1:previous_count$Freq[i]
}

purpose_count <- as.data.frame(table(purposeDF$id))
purpose_count <- purpose_count[order(purpose_count$Var1), ]
purposeDF <- purposeDF[order(purposeDF$id), ]
purposeDF$coder_num <- NA

for (i in 1:length(purpose_count$Freq)){
  purposeDF$coder_num[purposeDF$id == purpose_count$Var1[i]] <- 1:purpose_count$Freq[i]
}

# long to wide for scoring
previousDF_wide <- pivot_wider(data = previousDF,
                              id_cols = c("id", "survey_lang", "previous"), 
                              names_from = "coder_num",
                              values_from = "code")

purposeDF_wide <- pivot_wider(data = purposeDF,
                              id_cols = c("id", "survey_lang", "purpose"), 
                              names_from = "coder_num",
                              values_from = "code")

# score the yes/no/maybe/NA
previousDF_wide$score_1 <- previousDF_wide$`1`
previousDF_wide$score_2 <- previousDF_wide$`2`
previousDF_wide$score_3 <- previousDF_wide$`3`
previousDF_wide$score_4 <- previousDF_wide$`4`

previousDF_wide$score_1 <- gsub("maybe", "1", previousDF_wide$score_1)
previousDF_wide$score_1 <- gsub("no|na", "0", previousDF_wide$score_1)
previousDF_wide$score_1 <- gsub("yes|test", "2", previousDF_wide$score_1)
previousDF_wide$score_2 <- gsub("maybe", "1", previousDF_wide$score_2)
previousDF_wide$score_2 <- gsub("no|na", "0", previousDF_wide$score_2)
previousDF_wide$score_2 <- gsub("yes|test", "2", previousDF_wide$score_2)
previousDF_wide$score_3 <- gsub("maybe", "1", previousDF_wide$score_3)
previousDF_wide$score_3 <- gsub("no|na", "0", previousDF_wide$score_3)
previousDF_wide$score_3 <- gsub("yes|test", "2", previousDF_wide$score_3)
previousDF_wide$score_4 <- gsub("maybe", "1", previousDF_wide$score_4)
previousDF_wide$score_4 <- gsub("no|na", "0", previousDF_wide$score_4)
previousDF_wide$score_4 <- gsub("yes|test", "2", previousDF_wide$score_4)

previousDF_wide$count <- apply(
  previousDF_wide[ , grep("score", colnames(previousDF_wide))], 1, 
  function(x) { sum(!is.na(x)) } )
previousDF_wide$sum <- apply(
  previousDF_wide[ , grep("score", colnames(previousDF_wide))], 1, 
  function (x) { sum(as.numeric(x), na.rm = T)} )

previousDF_wide$exclude <- previousDF_wide$sum >= 4
previousDF_wide$nonsense <- apply(
  previousDF_wide[ , grep("1|2|3|4", colnames(previousDF_wide))], 1, 
  function (x) {sum(as.numeric(x == "na"), na.rm = T)}
) == 3

purposeDF_wide$score_1 <- purposeDF_wide$`1`
purposeDF_wide$score_2 <- purposeDF_wide$`2`
purposeDF_wide$score_3 <- purposeDF_wide$`3`

purposeDF_wide$score_1 <- gsub("maybe", "1", purposeDF_wide$score_1)
purposeDF_wide$score_1 <- gsub("no|na", "0", purposeDF_wide$score_1)
purposeDF_wide$score_1 <- gsub("yes|test", "2", purposeDF_wide$score_1)
purposeDF_wide$score_2 <- gsub("maybe", "1", purposeDF_wide$score_2)
purposeDF_wide$score_2 <- gsub("no|na", "0", purposeDF_wide$score_2)
purposeDF_wide$score_2 <- gsub("yes|test", "2", purposeDF_wide$score_2)
purposeDF_wide$score_3 <- gsub("maybe", "1", purposeDF_wide$score_3)
purposeDF_wide$score_3 <- gsub("no|na", "0", purposeDF_wide$score_3)
purposeDF_wide$score_3 <- gsub("yes|test", "2", purposeDF_wide$score_3)

purposeDF_wide$count <- apply(
  purposeDF_wide[ , grep("score", colnames(purposeDF_wide))], 1, 
  function(x) { sum(!is.na(x)) } )
purposeDF_wide$sum <- apply(
  purposeDF_wide[ , grep("score", colnames(purposeDF_wide))], 1, 
  function (x) { sum(as.numeric(x), na.rm = T)} )

purposeDF_wide$exclude <- purposeDF_wide$sum >= 4
purposeDF_wide$nonsense <- apply(
  purposeDF_wide[ , grep("1|2|3", colnames(purposeDF_wide))], 1, 
  function (x) {sum(as.numeric(x == "na"), na.rm = T)}
) == 3

sum(duplicated(purposeDF_wide$id))
#ids <- purposeDF_wide$id[duplicated(purposeDF_wide$id)]
#View(purposeDF_wide[purposeDF_wide$id %in% ids, ])
sum(duplicated(previousDF_wide$id))
#ids <- previousDF_wide$id[duplicated(previousDF_wide$id)]
#View(previousDF_wide[previousDF_wide$id %in% ids, ])
```

Finding missing coders:

```{r}
previous_missing <- subset(previousDF_wide, count < 3)
purpose_missing <- subset(purposeDF_wide, count < 3)

table(previous_missing$survey_lang)
table(purpose_missing$survey_lang)

#create reviewer names
previous_completed <- previousDF %>% 
  group_by(id) %>% 
  summarize(coder = paste(coder, collapse = ", "))

purpose_completed <- purposeDF %>% 
  group_by(id) %>% 
  summarize(coder = paste(coder, collapse = ", "))

previous_missing <- merge(previous_missing, 
                          previous_completed, 
                          by = "id", 
                          all.x = T)

purpose_missing <- merge(purpose_missing, 
                         purpose_completed,
                         by = "id", 
                         all.x = T)

write.csv(previous_missing, "../04_Data/data/previous_needs.csv", row.names = F)
write.csv(purpose_missing, "../04_Data/data/purpose_needs.csv", row.names = F)
```

```{r}
previousDF_long_missing <- subset(previousDF, id %in% previous_missing$id)
previousDF_wide_missing <- pivot_wider(data = previousDF_long_missing, 
                                       id_cols = c("id", "survey_lang"), 
                                       values_from = "code", 
                                       names_from = "coder")

purposeDF_long_missing <- subset(purposeDF, id %in% purpose_missing$id)
purposeDF_wide_missing <- pivot_wider(data = purposeDF_long_missing, 
                                       id_cols = c("id", "survey_lang"), 
                                       values_from = "code", 
                                       names_from = "coder")

write.csv(previousDF_wide_missing, "../04_Data/data/previous_needs_wide.csv", row.names = F)
write.csv(purposeDF_wide_missing, "../04_Data/data/purpose_needs_wide.csv", row.names = F)
```

## Fix Data Coding / Examine Covariates  

In this section, we clean up the following data issues:
  
  - Recode compensation to text data and fix missing mturk/other lab information ... note that if labs had a consistent compensation they were not shown that question
  - Examine gender labels
  - Recode education to number and examine ... fix issues with two labs not using the same education values. 

```{r}
# better labels for graphs
full_long$vignette <- factor(full_long$vignette,
                             levels = c("D", "E", "G"),
                             labels = c("Darrel", "Emma", "Gerald"))

full_long$cond <- factor(full_long$cond,
                             levels = c("G", "I", "K"),
                             labels = c("Gettier", "Ignorance", "Knowledge"))

# give compensation type labels
table(full_long$comp, useNA = "ifany")
full_long$comp <- factor(full_long$comp,
                         levels = 1:3,
                         labels = c("No", "Yes", "Not Sure"))

# examine compensation type by lab
table(full_long$lab_id, full_long$comp, useNA = "ifany")

# fix compensation for labs that didn't see that question
labs_compensated <- c("20a1ff", "1c0a06", "f6d5a9", "612ed8", "aba0ac", "2f579c") #this includes lottery compensation
labs_notcompensated <- c("cb060d", "c1e6b4", "c7b55a", "9c8799", "6898ef")

full_long$comp[full_long$lab_id %in% labs_compensated] <- "Yes"
full_long$comp[full_long$lab_id %in% labs_notcompensated] <- "No"

# this is mturk compensation is yes 
full_long$comp[full_long$lab_id == "b6c8ec"] <- "Yes"
full_long$turk <- full_long$lab_id == "b6c8ec"

table(full_long$lab_id, full_long$comp, useNA = "ifany")

# deal with not sures 
table(full_long$comp, useNA = "ifany")

full_long$comp[full_long$comp == "Not Sure" & is.na(full_long$comp_type)] <- "No"
full_long$comp[full_long$comp == "Not Sure" & !is.na(full_long$comp_type)] <- "Yes"

full_long$comp <- droplevels(full_long$comp)

# gender
table(full_long$gender, useNA = "ifany")

# describe education years
full_long$education <- as.numeric(full_long$education)
describe(full_long$education[!duplicated(full_long$id)])

levels(full_long$education_level)

# less high school = 10  
# high school = 12
# some college = 14
# college = 16
# masters = 18
# graduate = 20

levels(full_long$education) <- c(levels(full_long$education), 20, 21)

full_long$education[full_long$education_level == "Less than high school" & !is.na(full_long$education_level)] <- 10
full_long$education[full_long$education_level == "High school diploma (or GED)" & !is.na(full_long$education_level)] <- 12
full_long$education[full_long$education_level == "Some college or a 2-year college degree (A.A.)" & !is.na(full_long$education_level)] <- 14
full_long$education[full_long$education_level == "4-year college degree (B.A., B.S.)" & !is.na(full_long$education_level)] <- 16
full_long$education[full_long$education_level == "Master’s degree (M.A., M.S.)" & !is.na(full_long$education_level)] <- 18
full_long$education[full_long$education_level == "Graduate or professional degree (J.D., Ph.D., M.D.)" & !is.na(full_long$education_level)] <- 20

# PRT lab bach = 15, master = 17, PHD = 21
full_long$education[full_long$education_level == "4-year college degree (B.A., B.S.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <- 15
full_long$education[full_long$education_level == "Master’s degree (M.A., M.S.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <- 17
full_long$education[full_long$education_level == "Graduate or professional degree (J.D., Ph.D., M.D.)" & !is.na(full_long$education_level) & full_long$lab_id == "b6c8ec"] <- 21

# describe age
describe(full_long$age[!duplicated(full_long$id)])

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))
```

## Study Exclusions

In this section, we will mark each exclusion criteria to be able to denote how many exclusions each participant may have had.

### Majority Age

(1) if the participant is not the majority age of their country or older (unless parent/guardian waiver provided)

In this section, we will exclude all participants who are under 18 or are not the majority age of their country or older.

https://en.wikipedia.org/wiki/Age_of_majority

```{r}
# table of lab countries
table(full_long$lab_country, useNA = "ifany")

# age information
summary(full_long$age)

# minimum age by country
tapply(full_long$age, full_long$lab_country, min, na.rm = T)

# exclude based on age
full_long$age_exclusion <- NA

age18 <- c("AUT", "AUS", "CAN", "CHE", "DEU", "GBR", "GRC", "HUN", "NOR", "NZL", "POL", "PRT", "ROU", "RUS", "SGP", "TUR", "TWN", "USA")
age19 <- c("SVK")
age21 <- c("ARE")

full_long$age_exclusion[full_long$lab_country %in% age18] <- full_long$age[full_long$lab_country %in% age18] < 18

full_long$age_exclusion[full_long$lab_country %in% age19] <- full_long$age[full_long$lab_country %in% age19] < 19

full_long$age_exclusion[full_long$lab_country %in% age21] <- full_long$age[full_long$lab_country %in% age21] < 21

full_long$age_exclusion[is.na(full_long$age_exclusion)] <- TRUE

full_long$age_exclusion[full_long$age > 100] <- TRUE
```

### Previous Knowledge/Participation of Study 

(2) if the participant has taken part in a previous version of this study or in another contributors' replication of the same study

```{r}
full_long <- merge(full_long,
                   previousDF_wide[ , c("id", "exclude", "nonsense")], all.x = T) 

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))

colnames(full_long)[(ncol(full_long)-1):ncol(full_long)] <- c("previous_exclusion", "previous_nonsense")
```

### Comprehension Questions 

(3) if the participant fails to answer comprehension questions correctly

```{r}
table(full_long$dge_valid)

full_long$studyans_exclusion <- !(full_long$dge_valid)
```

### Knowledge of Experiment/Hypothesis 

(4) if the participant correctly and explicitly articulate knowledge of the specific hypotheses or specific conditions of this study when answering the funneled debriefing questions. 

```{r}
full_long <- merge(full_long,
                   purposeDF_wide[ , c("id", "exclude", "nonsense")], all.x = T) 

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))

colnames(full_long)[(ncol(full_long)-1):ncol(full_long)] <- c("purpose_exclusion", "purpose_nonsense")
```

### Language Proficiency 

We will also exclude participants who self-report their understanding of the tested language as "not well" or "not well at all". We based this exclusion criteria on a recent study that found that non-native English speakers who self-report as "very well" and "well" tend to score in the "intermediate" and "basic" categories on an English proficiency test respectively, while those who self-report as "not well" and "not at all" tend to score in the "below basic" category (Vickstrom, Shin, Collazo, & Bauman, 2015). All excluded data will be included in the data files on the overall OSF page, along with the particular reason for why they were excluded. 

```{r}
summary(full_long$language)

full_long$lang_exclusion <- full_long$language == "not very well" | full_long$language == "not well at all" 

full_long$lang_exclusion[is.na(full_long$lang_exclusion)] <- TRUE
```

### Total Exclusions

In this section, we provide the total (data point) exclusions for each particular screening item plus a table with all exclusions forever. 

```{r}
full_long$total_exclusion <- apply(full_long[ , grep("exclusion", colnames(full_long))], 1, sum)

table(full_long$age_exclusion)
table(full_long$lang_exclusion)
table(full_long$studyans_exclusion)
table(full_long$previous_exclusion)
table(full_long$purpose_exclusion)
table(full_long$total_exclusion)

# multiway table of all exclusions 
exclusions <- as.data.frame(table(full_long$age_exclusion, 
                                  full_long$lang_exclusion, 
                                  full_long$previous_exclusion,
                                  full_long$purpose_exclusion, 
                                  full_long$studyans_exclusion))
colnames(exclusions) <- c("Age", "Language", "Previous", "Purpose", "Comprehension", "Frequency")
exclusions$Num_Participants <- exclusions$Frequency / 3

exclusions

final_long <- subset(full_long, total_exclusion < 1)

# total number data points originally
nrow(full_long) 
# total number data points now
nrow(final_long) 

# original number of participants 
length(unique(full_long$id)) 
# final number of participants 
length(unique(final_long$id)) 
```

## Examine the Distributions

```{r}
table(final_long$bin_order)
final_long$bin_order <- droplevels(final_long$bin_order)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")
```

## Descriptives and Demograhics

```{r}
# total lab count
length(unique(full_long$lab_id))

# usable data lab count
length(unique(final_long$lab_id))

# total countries
table(full_long$lab_country)
length(table(full_long$lab_country))

# usable countries
table(final_long$lab_country)
length(table(final_long$lab_country))

# proportion usable data by country
table(final_long$lab_country) / table(full_long$lab_country)
```

## Analysis Overview 

- Analysis:
  - Multilevel modeling using appropriate linking function 
- Dependent variable(s):
  - Knowledge
  - Luck
  - Reasonable
- Independent variable:
  - Condition 
- Covariates: 
  - Test setting: compensated versus uncompensated
  - Age
  - Gender (men/women/other)
  - Years of education 
- Random intercepts:
  - participant / vignette / lab / maybe geographic region
- Special population:
  - Turk data on similar analysis 
  
## Data Screening

In this section, we will screen the continuous VAS variable, as the BIN variable does not require the same screening because of the different assumptions. 

### Knowledge 

```{r}
# drop levels
final_long$lab_id <- droplevels(final_long$lab_id)
final_long$id <- factor(final_long$id)
final_long$vignette <- factor(final_long$vignette)

# check numbers
table(final_long$cond, useNA = "ifany")
table(final_long$comp, useNA = "ifany")
table(final_long$gender, useNA = "ifany")
length(na.omit(final_long$know_vas))

# check data
hist(final_long$know_vas)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")

## knowledge 
ds.model <- lme(know_vas ~ cond + comp + age + gender + education,
                random = list(~1|vignette, ~1|id, ~1|lab_id),
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 
# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Reasonableness 

```{r}
# check data
hist(final_long$reason_vas)

# visualize the problem 
ggplot(final_long, aes(reason_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Reasonableness Attribution Visual Analogue Score")

## reasonable 
ds.model <- lme(reason_vas ~ cond + comp_type + age + 
                  gender + education,
                random = list(~1|vignette, ~1|id), #won't converge with lab_id
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 

# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Luck 

```{r}
# check data
hist(final_long$luck_vas)

# visualize the problem 
ggplot(final_long, aes(luck_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Luck Attribution Visual Analogue Score")

ds.model <- lme(luck_vas ~ cond + comp_type + age + 
                  gender + education ,
                random = list(~1|vignette, ~1|id),
                data = final_long,
                na.action = "na.omit")

# additivity
round(summary(ds.model)$corFixed,2)

# normality
standardized <- residuals(ds.model, type = "normalized")
hist(standardized) 

# linearity
{qqnorm(standardized)
  abline(0,1)}

# homosc
fitted <- scale(fitted.values(ds.model))
{plot(fitted, standardized)
  abline(v = 0)
  abline(h = 0)}
```

### Update DV  

In this instance, we do not see any form of linearity or homoscedasticity - the data is clearly skewed or bimodal. Therefore, we decided to split the VAS scale into 40 and below and 60 and above, then combine with the truly binary data, and include this variable to understand if there were differences when the VAS scale did not produce continuous data. 

Please note that the VAS is scored in the opposite direction of the BIN scales, therefore, you will see that the data split corrects this directionality difference. 

```{r}
## knowledge
table(final_long$know_bin)

## split data 
final_long$know_vas_binned <- final_long$know_vas
final_long$know_vas_binned[final_long$know_vas_binned <= 40] <- 2
final_long$know_vas_binned[final_long$know_vas_binned >= 40 & 
                             final_long$know_vas_binned <= 60] <- NA
final_long$know_vas_binned[final_long$know_vas_binned >= 60] <- 1
table(final_long$know_vas_binned, useNA = "ifany")

# no overlap
table(final_long$know_bin, final_long$know_vas_binned, useNA = "ifany")

final_long$know_vas_combined <- ifelse(is.na(final_long$know_vas_binned), 
                                     final_long$know_bin, 
                                     final_long$know_vas_binned)

final_long$know_vas_combined_source <- ifelse(is.na(final_long$know_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$know_vas_combined)

## reasonable
table(final_long$reason_bin)

## split data 
final_long$reason_vas_binned <- final_long$reason_vas
final_long$reason_vas_binned[final_long$reason_vas_binned <= 40] <- 2
final_long$reason_vas_binned[final_long$reason_vas_binned >= 40 & 
                             final_long$reason_vas_binned <= 60] <- NA
final_long$reason_vas_binned[final_long$reason_vas_binned >= 60] <- 1
table(final_long$reason_vas_binned, useNA = "ifany")

# no overlap
table(final_long$reason_bin, final_long$reason_vas_binned, useNA = "ifany")

final_long$reason_vas_combined <- ifelse(is.na(final_long$reason_vas_binned), 
                                     final_long$reason_bin, 
                                     final_long$reason_vas_binned)

final_long$reason_vas_combined_source <- ifelse(is.na(final_long$reason_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$reason_vas_combined)
table(final_long$reason_vas_combined_source)

## luck
table(final_long$luck_bin)

## split data 
final_long$luck_vas_binned <- final_long$luck_vas
final_long$luck_vas_binned[final_long$luck_vas_binned <= 40] <- 2
final_long$luck_vas_binned[final_long$luck_vas_binned >= 40 & 
                             final_long$luck_vas_binned <= 60] <- NA
final_long$luck_vas_binned[final_long$luck_vas_binned >= 60] <- 1
table(final_long$luck_vas_binned, useNA = "ifany")

# no overlap
table(final_long$luck_bin, final_long$luck_vas_binned, useNA = "ifany")

final_long$luck_vas_combined <- ifelse(is.na(final_long$luck_vas_binned), 
                                     final_long$luck_bin, 
                                     final_long$luck_vas_binned)

final_long$luck_vas_combined_source <- ifelse(is.na(final_long$luck_vas_binned), 
                                     "Binary", 
                                     "VAS")

table(final_long$luck_vas_combined)
```

### Gender Other

Because of the small other category, we will simply compare male to female. 

```{r}
final_long$gender2 <- factor(final_long$gender,
                             levels = c("female", "male"))
```

## Data Checks

### Missing Data Information 

```{r}
table(final_long$lab_id, final_long$vignette, useNA = "ifany")

table(final_long$lab_id, final_long$comp, useNA = "ifany")

table(final_long$lab_id, final_long$gender2, useNA = "ifany")

table(final_long$lab_id, final_long$cond, useNA = "ifany")

table(final_long$lab_id, final_long$know_vas_combined, useNA = "ifany")

table(final_long$lab_id, final_long$reason_vas_combined, useNA = "ifany")

table(final_long$lab_id, final_long$luck_vas_combined, useNA = "ifany")

table(final_long$lab_id, final_long$cond, useNA = "ifany")

table(final_long$lab_id, final_long$cond, useNA = "ifany")

tapply(final_long$education, final_long$lab_id, mean, na.rm = T)

tapply(final_long$age, final_long$lab_id, mean, na.rm = T)
```

### Check for Reverse Coding 

```{r}
table(final_long$bin_order)
final_long$bin_order <- droplevels(final_long$bin_order)

# visualize the problem 
ggplot(final_long, aes(know_vas)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Visual Analogue Score")

# visualize the problem 
ggplot(final_long, aes(know_bin)) + 
  geom_histogram() + 
  facet_grid(cond~vignette*bin_order) +  
  theme_classic() + 
  ylab("Frequency") + 
  xlab("Knowledge Attribution Bin Score")
```

## Final Analysis Knowledge

### Model 1 

Intercept only model to help determine if the addition of random intercepts are useful.

```{r}
#scores should be 0 and 1 for this analysis 
final_long$know_vas_combined <- final_long$know_vas_combined - 1

model.1 <- glm(know_vas_combined ~ 1,
               data = final_long,
               family = binomial,
               na.action = "na.omit")

summary(model.1)
```

### Model 2

Vignette random intercept model:

```{r}
model.2 <- glmer(know_vas_combined ~ (1|vignette),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.2)
coef(model.2) # log odds zero is the center 
exp(coef(model.2)$vignette) # regular odds with 1 at the center 
table(final_long$vignette, final_long$know_vas_combined)
r.squaredGLMM(model.2)
```

  - Remember that 0 is knows and 1 is believes 
  - Model 1 AIC: `r AIC(model.1)`
  - Model 2 AIC: `r AIC(model.2)`
  - Interpretation: the inclusion of the vignette random variable is useful
  - Coefficients: interpretation overall of random intercept, which effectively tells us regardless of condition, which way the participants lean toward answering. See the table printed out to help understand these values. 
    - Darrel: fairly even split for knows/believes with a slight favor to knows (because it's negative goes toward the 0 group)
    - Emma: leans more heavily toward the believes category (almost 4 to 1 odds)
    - Gerald: leans slightly toward the believes category
    - The results here indicate what we were seeing with the "continuous" data as well - that there's a big difference in the way they respond to vignettes
  - This vignette accounts for 8 to 10 percent of the random variance. 
  
### Model 3

Vignette nested in participants random intercept:

```{r}
model.3 <- glmer(know_vas_combined ~ (1|vignette/id),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.3)
r.squaredGLMM(model.3)
```


  - Model 2 AIC: `r AIC(model.2)`
  - Model 3 AIC: `r AIC(model.3)`
  - Interpretation: the inclusion of participant ID isn't really useful, as the AIC is higher for this model. The variance between participants is very small after accounting for vignettes.
  - This model accounts for approximately the same random variance. 
  
### Model 4

Vignette nested in participants which is then nested in labs:

```{r}
model.4 <- glmer(know_vas_combined ~ (1|vignette/id/lab_id),
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.4)
r.squaredGLMM(model.4)
```

  - Model 2 AIC: `r AIC(model.2)`
  - Model 3 AIC: `r AIC(model.3)`
  - Model 4 AIC: `r AIC(model.4)`
  - Interpretation: the inclusion of lab ID isn't really useful, as the AIC is higher for this model. The variance parameter for lab is also very small. 
  - This model accounts for approximately the same random variance.
  - We will continue to use this cross classified structure to ensure we account for correlated error and to follow the pre-print.
  
### Model 5

Addition of the covariates added to the vignette random intercept model. 

```{r}
model.5 <- glmer(know_vas_combined ~ (1|vignette/id/lab_id) + 
                   comp + age + gender2 + education,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.5)

r.squaredGLMM(model.5)
```

  - Model 4 AIC: `r AIC(model.4)`
  - Model 5 AIC: `r AIC(model.5)`
  - Interpretation: this model is better than the vignette random intercept only model.
  - No covariates predicted the outcome variable. 
  
### Model 6

The previous model of covariates plus the variable of focus: condition.

```{r}
model.6 <- glmer(know_vas_combined ~ (1|vignette/id/lab_id) + 
                   comp + age + gender2 + education + 
                   cond,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.6)

table(model.6@frame$cond, model.6@frame$know_vas_combined)

r.squaredGLMM(model.6)
```

  - Model 5 AIC: `r AIC(model.5)`
  - Model 6 AIC: `r AIC(model.6)`
  - Interpretation: this model is better than the model with the covariates. 
  - Both knowledge and ignorance are different than the Gettier condition.
  - In Gettier, participants are slightly more likely to say believes.
  - In Ignorance, participants are *way* more likely to say believes (so that's why this condition is different from Gettier).
  - In Knowledge, participants are more like to say knows (also why this is different from Gettier). 
  - This effect accounts for 13-15% of the fixed effect variance. 

### Model 6A

Does the vignette interact with the condition?

```{r}
model.6A <- glmer(know_vas_combined ~ (1 | vignette/id/lab_id) + 
                   comp + age + gender2 + education + 
                   cond*vignette,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.6A)

r.squaredGLMM(model.6A)

data_graph <- model.6A@frame
data_graph$know_vas_combined <- factor(data_graph$know_vas_combined,
                                          levels = c(0,1),
                                          labels = c("Knows", "Believes"))

#graph the three way binary 
ggplot(data_graph) +
  geom_mosaic(aes(x = product(know_vas_combined, cond, vignette), 
                  fill = know_vas_combined)) + 
  scale_fill_manual(name = "Knowledge Choice",
                    values = c("gray", "black")) + 
  scale_x_productlist(breaks = c(0.13,.5,.87),
    labels = c("Darrel", "Emma", "Gerald")) + 
  theme_classic() + 
  xlab("Vignette") + 
  ylab("Condition")
```

  - Model 6 AIC: `r AIC(model.6)`
  - Model 6A AIC: `r AIC(model.6A)`
  - Interpretation: This model is better than a model without the interaction. 
  - There is an interaction between vignette and condition for choice. 
  - In looking at the graph, we can see this is driven by the Emma vignette which shows a larger believes box for all conditions. 
  - This interaction adds a good portion of variance to the fixed effects 7-9%. 

To analyze this interaction, we will use chi-square analysis and proportion tests (making sure it's on the same data as the analysis above).

```{r}
# overall darrel
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Darrel"], 
           data_graph$cond[data_graph$vignette == "Darrel"])
d_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Darrel"], 
           data_graph$cond[data_graph$vignette == "Darrel"])
d_table

# prop tests
prop.test(d_table[1:2, 1:2])
prop.test(d_table[1:2, c(1,3)])

# overall emma
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Emma"], 
           data_graph$cond[data_graph$vignette == "Emma"])
e_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Emma"], 
           data_graph$cond[data_graph$vignette == "Emma"])
e_table

# prop tests
prop.test(e_table[1:2, 1:2])
prop.test(e_table[1:2, c(1,3)])

# overall gerald
chisq.test(data_graph$know_vas_combined[data_graph$vignette == "Gerald"], 
           data_graph$cond[data_graph$vignette == "Gerald"])
g_table <- table(data_graph$know_vas_combined[data_graph$vignette == "Gerald"], 
           data_graph$cond[data_graph$vignette == "Gerald"])
g_table

# prop tests
prop.test(e_table[1:2, 1:2])
prop.test(e_table[1:2, c(1,3)])
```


### Model 6B

Data was collected from mechanical turk participants. Are there differences in this data and the complete data on the condition variable? 

```{r}
model.6B <- glmer(know_vas_combined ~ (1|vignette/id/lab_id) + 
                    comp + age + gender2 + education + 
                    cond*turk,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.6B)

r.squaredGLMM(model.6B)
```

  - Model 6 AIC: `r AIC(model.6)`
  - Model 6A AIC: `r AIC(model.6B)`
  - Interpretation: This model is better than a model without the interaction. 
  - However, the interaction with the turk variable does not produce significant effects.
  - The model is likely "better" because of the added variable and main effect of turk (likely drive by sample size differences in the two samples of turk versus not for knows versus believes). 
  - Therefore, we do not find evidence for differences in the turk sample for condition effects. 

### Model 6C

Data was binned because of the distributions of the visual analogue scale - does the data source make a difference? 

```{r}
model.6C <- glmer(know_vas_combined ~ (1|vignette/id/lab_id) + 
                    comp + age + gender2 + education + 
                    cond*know_vas_combined_source,
                      data = final_long,
                      family = binomial,
                      control = glmerControl(optimizer = "bobyqa"),
                      nAGQ = 0)
summary(model.6C)

r.squaredGLMM(model.6C)
```

  - Model 6 AIC: `r AIC(model.6)`
  - Model 6C AIC: `r AIC(model.6C)`
  - Interpretation: This model is not better than a model without the condition.
  - There is no evidence for an interaction between condition and the data source, indicating that the binning process did not give us evidence of different results. 

