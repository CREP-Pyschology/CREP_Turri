---
title: "ACREP Analysis"
date: "Last Updated: `r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

valid is who answered comp questions correctly 

[Codebook](https://docs.google.com/spreadsheets/d/1KjXqgfVgguHeDXVtlHHhJ9zsRGVDfPVPH4tbh75P46U/edit#gid=903093128)

[Preprint](https://psyarxiv.com/zeux9/)

## Libraries

```{r}
library(rio)
library(lme4)
library(lmerTest)
library(MuMIn)
```

## Import the Data

The `full_long` dataset includes all participants in long format - wherein each trial of their study is on one row of the dataset. Our uploaded data also includes `full.Rds` which is the same data in wide format - one column for each variable in the dataset and one row per participant.

```{r}
full_long <- readRDS("full_long.Rds")
#str(full_long)
```

## Compute Variables

In this section, we break apart the vignette variable so we know which vignette is tied to each line of data to use in our multilevel analysis. 

```{r}
full_long$vignette <- NA
full_long$vignette[full_long$name_order != "" & !is.na(full_long$name_order)] <- unlist(strsplit(paste0(full_long$name_order[!duplicated(full_long$id) & full_long$name_order != "" & !is.na(full_long$name_order)], collapse = ""), ""))
```

## Recoding Information

Import from google doc jordan sent 

https://docs.google.com/spreadsheets/d/1UXc5L_wCFTISRzWj6uMVGLPag1bUEcNlX-yAh2oIQUI/edit#gid=544982568

```{r}

```

## Data Exclusions

Participants in any laboratory must be excluded for any one of the following reasons: 

(1) if the participant is not the majority age of their country or older (unless parent/guardian waiver provided)

In this section, we will exclude all participants who are under 18 or are not the majority age of their country or older.

https://en.wikipedia.org/wiki/Age_of_majority

```{r}
summary(full_long$age)

#use country from recoded stuff from above 
tapply(full_long$age, full_long$country, min, na.rm = T)

# number of rows
nrow(full_long)
# number of participants
length(unique(full_long$id))

#change here based on whatever you decide for countries 
valid_data <- subset(full_long, age >= 18)

# number of rows
nrow(valid_data)
# number of participants
length(unique(valid_data$id))
```

(2) if the participant has taken part in a previous version of this study or in another contributors' replication of the same study

```{r}
# pull in dictionary 
# subset using dictionary 

# number of rows
nrow(valid_data)
# number of participants
length(unique(valid_data$id))
```

(3) if the participant fails to answer comprehension questions correctly

```{r}
valid_data$urn1 <- factor(valid_data$urn1, levels = c("", "1"), labels = c("Not included", "Included"))
summary(valid_data$urn1)

valid_data <- subset(valid_data, urn1 == "Included")

# number of rows
nrow(valid_data)
# number of participants
length(unique(valid_data$id))
```

(4) if the participant correctly and explicitly articulate knowledge of the specific hypotheses or specific conditions of this study when answering the funneled debriefing questions. 

```{r}


# number of rows
nrow(valid_data)
# number of participants
length(unique(valid_data$id))
```

We will also exclude participants who self-report their understanding of the tested language as "not well" or "not well at all". We based this exclusion criteria on a recent study that found that non-native English speakers who self-report as "very well" and "well" tend to score in the "intermediate" and "basic" categories on an English proficiency test respectively, while those who self-report as "not well" and "not at all" tend to score in the "below basic" category (Vickstrom, Shin, Collazo, & Bauman, 2015). All excluded data will be included in the data files on the overall OSF page, along with the particular reason for why they were excluded. 

```{r}
summary(valid_data$language)

valid_data <- subset(valid_data, language != "not very well" | language != "not well at all" | language != "Not answered")

summary(valid_data$language)

# number of rows
nrow(valid_data)
# number of participants
length(unique(valid_data$id))
```

do the exclusions by marking them and then create a total-total of who is getting excluded 

## Descriptive Statistics

In total, [X] labs applied to participate in this multilab replication. [X] labs were unable to participate, [X] did not collect enough data; [X] dropped out prior to data collection, resulting in a final lab count of [X]. Contributing labs represent [X] continents ([X from Africa, X from South America, X from North America, X from Asia, X from Europe, and X from Oceania) with participants residing in [X] countries [X from Brazil, X from Switzerland, X from Singapore, and so on]. [X labs committed to collecting the minimum participant sample size (N = 50), and X labs committed to collecting a larger, more representative sample (N = 100) for the purposes of exploratory analyses. 

```{r}

# number of labs - final
length(unique(valid_data$lab_id))

# number of participants for each lab
table(valid_data$lab_id[duplicated(valid_data$id)])
```

## Analysis

### Knowledge

To assess the model fit of our data, we used the commonly used nested model test using maximum likelihood estimation (Snijders & Bosker, 2012). Next, we wanted to determine if the effects of the primary independent variable (belief condition) on knowledge attribution differed by vignette, participant, or lab. To accomplish this, we built an unconditional base model for the knowledge attribution predictor to calculate the intra-class correlation coefficients (ICC) for vignette, participant, and lab variation. The ICCs for vignettes, participants, and labs in the dataset measures the percentage of variation explained by each level, such that vignettes accounted for [X.XX%, 95% CI [X.XX, X.XX] of the raw variation in the dataset, participants accounted for [X.XX%, 95% CI [X.XX, X.XX] of the raw variation in the dataset, and labs accounted for [X.XX%, 95% CI [X.XX, X.XX] of the raw variation in the dataset. 

```{r}
# ICCs vignette
knowledge_vignette <- glmer(var_know_bin ~ (1 | vignette), 
                         data = valid_data,
                         family = binomial, 
                         control = glmerControl(optimizer = "bobyqa"),
                         nAGQ = 1)

summary(knowledge_vignette)

table(valid_data$vignette, valid_data$var_know_bin)

r.squaredGLMM(knowledge_vignette)

# ICCs participants
knowledge_id <- glmer(var_know_bin ~ (1 | id), 
                         data = valid_data,
                         family = binomial, 
                         control = glmerControl(optimizer = "bobyqa"),
                         nAGQ = 1)

summary(knowledge_id)

r.squaredGLMM(knowledge_id)

# ICCs labs
knowledge_lab_id <- glmer(var_know_bin ~ (1 | lab_id), 
                         data = valid_data,
                         family = binomial, 
                         control = glmerControl(optimizer = "bobyqa"),
                         nAGQ = 1)

summary(knowledge_lab_id)

r.squaredGLMM(knowledge_lab_id)


```

chisquare then if the chisquare is significant do the pearsons correlation 
- if the goal is to say they are the same pattern then use the pearson
- if the goal is to say they are different then use chisquare 

```{r}
table(valid_data$var_know_bin, valid_data$vignette)
chisq.test(valid_data$var_know_bin, valid_data$vignette)
```



Given that we are primarily interested in the relationship between the hypothesized level-2 between-subjects predictor (X1) and the two hypothesized outcome variables (knowledge, Y1; and reasonableness, Y2), we first performed the analysis using solely the primary hypothesized independent variable (belief condition) without any other covariates for the purpose of trying to estimate the overall individual level effect (fixed slope) on the primary hypothesized outcome (i.e., null model). In other terms, we determined the effect on knowledge attribution across all samples, not accounting for covariates, vignette differences, or lab differences. We found that the overall effect of belief condition was [insignificant/small/medium/large, Î² = .XX, 95% CIs [X.XX, X.XX]]. We then tested the model fit for each analysis using likelihood ratio (LR) chi-square difference tests to determine whether each unit level should be tested as a random or fixed factor and whether covariates improved the model (Gelman & Hill, 2007, Chapter 17; see Table 1).

Within these models, vignette was tested as a random (within-subjects) factor, condition was tested as a fixed (between-subjects) factor, and labs were tested as a random (between-subjects) factor. We also fitted these models with several exploratory covariates, including participant gender, years of education, age, and three test setting lab variables (online vs. in person; in group vs. individually; compensated or not compensated).

knowledge_null <- glmer(var_know_bin ~ 1 + (1 | vignette) + 
                           (1 | lab_id) + (1 | lab_id / id), 
                         data = valid_data,
                         family = binomial, 
                         control = glmerControl(optimizer = "bobyqa"),
                         nAGQ = 1)


knowledge_model <- glmer(var_know_bin ~ cond + (1 | vignette) + 
                           (1 | lab_id) + (1 | lab_id / id), 
                         data = valid_data,
                         family = binomial, 
                         control = glmerControl(optimizer = "bobyqa"),
                         nAGQ = 1)

summary(knowledge_model)

table(valid_data$cond, valid_data$var_know_bin)

r.squaredGLMM(knowledge_model)
```

```{r}
knowledge_model2 <- lmer(var_know_vas ~ cond + (1 | vignette) + 
                           (1 | lab_id) + (1 | lab_id / id), 
                    data = valid_data,
                    na.action = "na.omit")

summary(knowledge_model2)

emmeans(knowledge_model2, "cond")

r.squaredGLMM(knowledge_model2)
```

### Reasonableness Attribution


random effects:
- vignette 
- lab
- participant 

fixed effects:
- condition
- test setting
- education
- gender
- country (may code this several ways) 